{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af46dbe",
   "metadata": {},
   "source": [
    "# TREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36182d98-e1a5-4640-934a-b125c9572202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n",
      "(5452, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>how did serfdom develop in and then leave russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what films featured the character popeye doyle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>how can i find a list of celebrities ' real na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what fowl grabs the spotlight after the chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>what is the full form of .com ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label-coarse  label-fine                                               text\n",
       "0             0           0  how did serfdom develop in and then leave russ...\n",
       "1             1           1   what films featured the character popeye doyle ?\n",
       "2             0           0  how can i find a list of celebrities ' real na...\n",
       "3             1           2  what fowl grabs the spotlight after the chines...\n",
       "4             2           3                    what is the full form of .com ?"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df=pd.read_csv(\"/Users/zhanglikang/Desktop/SC4002_G06/datasets/TREC/train.csv\")\n",
    "df[\"text\"] = df[\"text\"].str.lower()\n",
    "print(len(df))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c645e2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4952, 500)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data to create a development set of 500 examples\n",
    "train_data, dev_data = train_test_split(df, test_size=500, random_state=42)\n",
    "\n",
    "# Display the size of the training and development sets\n",
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3afa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OTHERS', 0, 5, 4, 3], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique coarse labels\n",
    "unique_labels = train_data['label-coarse'].unique()\n",
    "\n",
    "# Randomly select 4 classes\n",
    "selected_labels = np.random.choice(unique_labels, size=4, replace=False)\n",
    "\n",
    "# Update the labels in the train and dev sets\n",
    "train_data = train_data.copy()\n",
    "dev_data = dev_data.copy()\n",
    "\n",
    "train_data['new_label'] = train_data['label-coarse'].apply(lambda x: x if x in selected_labels else 'OTHERS')\n",
    "dev_data['new_label'] = dev_data['label-coarse'].apply(lambda x: x if x in selected_labels else 'OTHERS')\n",
    "\n",
    "# Display the unique labels in the updated training set\n",
    "train_data['new_label'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed134cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['new_label'] = train_data['new_label'].astype(str)\n",
    "dev_data['new_label'] = dev_data['new_label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb5b310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "# Download the \"glove-twitter-25\" embeddings\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "# retrieve the vector for 'computer'\n",
    "# glove_vectors['computer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b857be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>what is mikhail gorbachev 's middle initial ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>how does the tail affect the flight of a kite ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>what were the first three cities to have a pop...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the movie jonathan livingstone seagull ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>what is a fear of home surroundings ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse  label-fine  \\\n",
       "4943             2          34   \n",
       "2346             0           0   \n",
       "1835             5          21   \n",
       "4047             1           1   \n",
       "5097             1          23   \n",
       "\n",
       "                                                   text new_label  \n",
       "4943      what is mikhail gorbachev 's middle initial ?    OTHERS  \n",
       "2346    how does the tail affect the flight of a kite ?         0  \n",
       "1835  what were the first three cities to have a pop...         5  \n",
       "4047   what is the movie jonathan livingstone seagull ?    OTHERS  \n",
       "5097              what is a fear of home surroundings ?    OTHERS  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa3774e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>what city is served by tempelhol airport ?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is dudley do-right 's horse 's name ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>what 's nature 's purpose for tornadoes ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>what is the history of valentine 's day cards ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what president became chief justice after his ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse  label-fine  \\\n",
       "3408             5          21   \n",
       "371              1           2   \n",
       "453              0           9   \n",
       "290              0          12   \n",
       "4457             3           4   \n",
       "\n",
       "                                                   text new_label  \n",
       "3408         what city is served by tempelhol airport ?         5  \n",
       "371          what is dudley do-right 's horse 's name ?    OTHERS  \n",
       "453           what 's nature 's purpose for tornadoes ?         0  \n",
       "290     what is the history of valentine 's day cards ?         0  \n",
       "4457  what president became chief justice after his ...         3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b57d6b",
   "metadata": {},
   "source": [
    "\n",
    "## Use a simple linear layer + Averaging all word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d9da662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f5dd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to input vectors using glove_vectors\n",
    "def text_to_vectors(text, glove_vectors):\n",
    "    return [glove_vectors[w] for w in text.split() if w in glove_vectors]\n",
    "\n",
    "X_train = np.array([text_to_vector(text, glove_vectors) for text in train_data['text']])\n",
    "X_dev = np.array([text_to_vector(text, glove_vectors) for text in dev_data['text']])\n",
    "\n",
    "# Convert labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data['new_label'])\n",
    "y_dev = label_encoder.transform(dev_data['new_label'])\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.int64)\n",
    "X_dev, y_dev = torch.tensor(X_dev, dtype=torch.float32), torch.tensor(y_dev, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ad362a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4952, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "640ab63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(QuestionClassifier, self).__init__()\n",
    "        # Hidden Layer\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        # Output Layer\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # List to hold the outputs of the hidden layer for each word in a question\n",
    "#         outputs = []\n",
    "        \n",
    "#         # Process each word through the hidden layer\n",
    "#         for word_vec in x:\n",
    "#             word_vec = torch.tensor(word_vec, dtype=torch.float32)\n",
    "#             hidden_output = self.hidden(word_vec)\n",
    "#             outputs.append(hidden_output)\n",
    "        \n",
    "#         # Aggregation Layer: Averaging outputs of the hidden layer\n",
    "#         avg_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "        \n",
    "#         # Output Layer\n",
    "#         final_output = self.output(avg_output)\n",
    "#         return self.softmax(final_output)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "    # List to hold the outputs of the hidden layer for each word in a question\n",
    "        outputs = []\n",
    "\n",
    "        # Process each word through the hidden layer\n",
    "        for word_vec in x:\n",
    "\n",
    "            print(word_vec.shape)\n",
    "\n",
    "            word_vec = torch.tensor(word_vec, dtype=torch.float32).view(1, -1)  # Reshape the word vector\n",
    "            hidden_output = self.hidden(word_vec)\n",
    "            outputs.append(hidden_output)\n",
    "\n",
    "        # Aggregation Layer: Averaging outputs of the hidden layer\n",
    "        avg_output = torch.mean(torch.stack(outputs), dim=0).squeeze()  # Make sure the tensor shape is consistent\n",
    "\n",
    "        # Output Layer\n",
    "        final_output = self.output(avg_output)\n",
    "        return self.softmax(final_output)\n",
    "\n",
    "\n",
    "input_dim = 300  # 300 for 'word2vec-google-news-300'\n",
    "hidden_dim = 100\n",
    "output_dim = len(train_data['new_label'].unique())\n",
    "model = QuestionClassifier(input_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "903fbf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/0w6c56l564jgq2sjnc193_rw0000gn/T/ipykernel_16560/2608824258.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word_vec = torch.tensor(word_vec, dtype=torch.float32).view(1, -1)  # Reshape the word vector\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 300x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, question \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X_train):\n\u001b[0;32m---> 11\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), y_train[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     13\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 38\u001b[0m, in \u001b[0;36mQuestionClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(word_vec\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     37\u001b[0m     word_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(word_vec, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape the word vector\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     hidden_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(hidden_output)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Aggregation Layer: Averaging outputs of the hidden layer\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 300x100)"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, question in enumerate(X_train):\n",
    "        predictions = model(question)\n",
    "        loss = criterion(predictions.unsqueeze(0), y_train[i].unsqueeze(0))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward(retain_graph=True)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Evaluate on development set\n",
    "    model.eval()\n",
    "    total_dev_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, question in enumerate(X_dev):\n",
    "            dev_predictions = model(question)\n",
    "            dev_loss = criterion(dev_predictions.unsqueeze(0), y_dev[i].unsqueeze(0))\n",
    "            total_dev_loss += dev_loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(dev_predictions.data, 0)\n",
    "            correct += (predicted == y_dev[i]).item()\n",
    "\n",
    "    dev_accuracy = correct / len(y_dev)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} => \"\n",
    "          f\"Train Loss: {total_loss/len(X_train):.4f}, \"\n",
    "          f\"Dev Loss: {total_dev_loss/len(X_dev):.4f}, \"\n",
    "          f\"Dev Accuracy: {dev_accuracy:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539a72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839668cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the average word vector for a sentence\n",
    "def sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    # Split the sentence into words.\n",
    "\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    # Get the word vector for each word in the sentence if it exists in glove_vectors.\n",
    "\n",
    "\n",
    "    if len(vectors) == 0: # to avoid empty lists\n",
    "        return np.zeros(300)\n",
    "    # If no words from the sentence are in the word vectors, return a vector of zeros.\n",
    "\n",
    "    return np.mean(vectors, axis=0)\n",
    "# Return the average word vector for the sentence.\n",
    "\n",
    "train_data['avg_vector'] = train_data['text'].apply(sentence_vector)\n",
    "# Compute the average word vector for each sentence in the training data.\n",
    "\n",
    "X_train = np.vstack(train_data['avg_vector'].values)\n",
    "# Stack the average vectors to form the training data.\n",
    "\n",
    "y_train = pd.get_dummies(train_data['new_label']).values\n",
    "# Convert the new labels to one-hot encoded vectors.\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# Convert the training data to a PyTorch tensor.\n",
    "\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.int64)\n",
    "# Convert the one-hot encoded labels to their corresponding class indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2d981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network Model with a simple linear layer\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        # Initialize the parent class.\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        # Define a fully connected layer.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    # Define the forward pass to return the output of the linear layer.\n",
    "\n",
    "input_dim = 300  # as we're using word2vec-google-news-300\n",
    "# Define the input dimension based on the word vector size.\n",
    "\n",
    "output_dim = 5  # for our 5 new classes\n",
    "# Define the output dimension based on the number of new classes.\n",
    "\n",
    "model = SimpleClassifier(input_dim, output_dim)\n",
    "# Initialize the model.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the loss function (cross entropy).\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define the optimizer (Adam) with a learning rate.\n",
    "\n",
    "# Training\n",
    "epochs = 5000\n",
    "# Set the number of epochs.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "  \n",
    "\n",
    "\n",
    "    # Store metrics for visualization\n",
    "    all_metrics['A']['epochs'].append(epoch)\n",
    "    all_metrics['A']['accuracy'].append(accuracy)\n",
    "    all_metrics['A']['loss'].append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "    # Print the loss and accuracy for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2376da",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(all_metrics['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1671e",
   "metadata": {},
   "source": [
    "## Use a feedforward network which is a combination of a linear transformation and a nonlinear activation function\n",
    "\n",
    "## Max pooling over the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    if len(vectors) == 0: # if no words in the sentence have embeddings\n",
    "        return np.zeros(300)\n",
    "    return np.max(vectors, axis=0) # max pooling across the words\n",
    "\n",
    "train_data['maxpooled_vector'] = train_data['text'].apply(sentence_vector)\n",
    "\n",
    "X_train = np.vstack(train_data['maxpooled_vector'].values)\n",
    "y_train = pd.get_dummies(train_data['new_label']).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08402d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7b651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network model with feedforward layers\n",
    "class FeedForwardClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedForwardClassifier, self).__init__()\n",
    "        \n",
    "        # First linear layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Second linear layer that outputs class probabilities\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply first linear transformation\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply ReLU activation function\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        # Apply second linear transformation\n",
    "        return self.fc2(x)\n",
    "\n",
    "input_dim = 300  # as we're using word2vec-google-news-300\n",
    "hidden_dim = 1000  # can be adjusted based on performance\n",
    "output_dim = 5   # for our 5 new classes\n",
    "\n",
    "model = FeedForwardClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "\n",
    "    # Store metrics for visualization\n",
    "    all_metrics['B']['epochs'].append(epoch)\n",
    "    all_metrics['B']['accuracy'].append(accuracy)\n",
    "    all_metrics['B']['loss'].append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(all_metrics['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a3431",
   "metadata": {},
   "source": [
    "##  Recurrent neural network\n",
    "##  Aggregation Layer: Taking the representation of the last word (useful if using RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_matrix(sentence, max_len=30):\n",
    "    words = sentence.split()[:max_len]  # truncate if necessary\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    while len(vectors) < max_len:  # pad if necessary\n",
    "        vectors.append(np.zeros(300))\n",
    "    return np.array(vectors)\n",
    "\n",
    "train_data['vector_matrix'] = train_data['text'].apply(sentence_matrix)\n",
    "\n",
    "X_train = np.stack(train_data['vector_matrix'].values)\n",
    "y_train = pd.get_dummies(train_data['new_label']).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef05f99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network model with an RNN layer\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Linear layer that outputs class probabilities\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the RNN layer\n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass the final output through the linear layer\n",
    "        return self.fc(out)\n",
    "\n",
    "input_dim = 300  # as we're using word2vec-google-news-300\n",
    "hidden_dim = 100  # can be adjusted based on performance\n",
    "output_dim = 5   # for our 5 new classes\n",
    "\n",
    "model = RNNClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "\n",
    "    # Store metrics for visualization\n",
    "    all_metrics['C']['epochs'].append(epoch)\n",
    "    all_metrics['C']['accuracy'].append(accuracy)\n",
    "    all_metrics['C']['loss'].append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(all_metrics['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9f9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75e4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d457503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cc0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
