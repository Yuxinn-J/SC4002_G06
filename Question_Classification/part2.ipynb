{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af46dbe",
   "metadata": {},
   "source": [
    "# TREC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36182d98-e1a5-4640-934a-b125c9572202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e1110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n",
      "(5452, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"/Users/zhanglikang/Desktop/SC4002_G06/datasets/TREC/train.csv\")\n",
    "df_train[\"text\"] = df_train[\"text\"].str.lower()\n",
    "print(len(df_train))\n",
    "print(df_train.shape)\n",
    "df_train.head()\n",
    "\n",
    "df_test=pd.read_csv(\"/Users/zhanglikang/Desktop/SC4002_G06/datasets/TREC/test.csv\")\n",
    "df_test[\"text\"] = df_test[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c645e2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4952, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data to create a development set of 500 examples\n",
    "train_data, dev_data = train_test_split(df_train, test_size=500, random_state=42)\n",
    "test_data = df_test\n",
    "\n",
    "# Display the size of the training and development sets\n",
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc6463",
   "metadata": {},
   "source": [
    "## Update Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb5b310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "# Download the \"glove-twitter-25\" embeddings\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "# retrieve the vector for 'computer'\n",
    "# glove_vectors['computer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ac7f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether <UNK> in w2v: False\n",
      "whether <PAD> in w2v: False\n",
      "word2idx['<UNK>']: 3000000\n",
      "word2idx['<PAD>']: 3000001\n"
     ]
    }
   ],
   "source": [
    "# Out-of-vocabulary (OOV) words\n",
    "# 1. can be replaced with a special token, such as \"<OOV>\" or \"<UNK>\".\n",
    "# 2. can be ignored.\n",
    "\n",
    "word2idx = w2v.key_to_index\n",
    "print(f\"whether <UNK> in w2v: {'<UNK>' in word2idx}\") # False\n",
    "print(f\"whether <PAD> in w2v: {'<PAD>' in word2idx}\") # False\n",
    "\n",
    "# Add '<UNK>' and '<PAD>' tokens to the vocabulary index\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "print(f\"word2idx['<UNK>']: {word2idx['<UNK>']}\")\n",
    "print(f\"word2idx['<PAD>']: {word2idx['<PAD>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0585119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(w2v.vectors.shape)\n",
    "w2v['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652fc158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after insert UNK:  (3000001, 300)\n",
      "after insert UNK:  (3000002, 300)\n"
     ]
    }
   ],
   "source": [
    "# add the '<UNK>' word to the vocabulary of the Word2Vec model \n",
    "# initialize it with the average of all word vectors int he pretrained embeddings.\n",
    "unk_vector = np.mean(w2v.vectors, axis=0)\n",
    "w2v.vectors = np.vstack([w2v.vectors, unk_vector])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n",
    "\n",
    "# add the '<PAD>' word to the vocabulary of the Word2Vec model \n",
    "# initialize it with a row of zeros in the vectors matrix.\n",
    "w2v.vectors = np.vstack([w2v.vectors, np.zeros(w2v.vectors[0].shape)])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0970f69",
   "metadata": {},
   "source": [
    "## Modify Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3afa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 'OTHERS', 4, 3], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique coarse labels\n",
    "unique_labels = train_data['label-coarse'].unique()\n",
    "\n",
    "# Randomly select 4 classes\n",
    "np.random.seed(19260817)\n",
    "selected_labels = np.random.choice(unique_labels, size=4, replace=False)\n",
    "\n",
    "# ****** 6 == OTHERS !!!!!! IMPORTANT \n",
    "# update: 6 will cause error, change back to OTHERS then transform later\n",
    "\n",
    "train_data['new_label'] = train_data['label-coarse'].apply(lambda x: x if x in selected_labels else \"OTHERS\")\n",
    "dev_data['new_label'] = dev_data['label-coarse'].apply(lambda x: x if x in selected_labels else \"OTHERS\")\n",
    "test_data['new_label'] = test_data['label-coarse'].apply(lambda x: x if x in selected_labels else \"OTHERS\")\n",
    "\n",
    "# Display the unique labels in the updated training set\n",
    "train_data['new_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b857be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>what is mikhail gorbachev 's middle initial ?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>how does the tail affect the flight of a kite ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>what were the first three cities to have a pop...</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>what is the movie jonathan livingstone seagull ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>what is a fear of home surroundings ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse  label-fine  \\\n",
       "4943             2          34   \n",
       "2346             0           0   \n",
       "1835             5          21   \n",
       "4047             1           1   \n",
       "5097             1          23   \n",
       "\n",
       "                                                   text new_label  \n",
       "4943      what is mikhail gorbachev 's middle initial ?         2  \n",
       "2346    how does the tail affect the flight of a kite ?         0  \n",
       "1835  what were the first three cities to have a pop...    OTHERS  \n",
       "4047   what is the movie jonathan livingstone seagull ?    OTHERS  \n",
       "5097              what is a fear of home surroundings ?    OTHERS  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fabe8d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>what city is served by tempelhol airport ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>what is dudley do-right 's horse 's name ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>what 's nature 's purpose for tornadoes ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>what is the history of valentine 's day cards ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>what president became chief justice after his ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse  label-fine  \\\n",
       "3408             5          21   \n",
       "371              1           2   \n",
       "453              0           9   \n",
       "290              0          12   \n",
       "4457             3           4   \n",
       "\n",
       "                                                   text new_label  \n",
       "3408         what city is served by tempelhol airport ?    OTHERS  \n",
       "371          what is dudley do-right 's horse 's name ?    OTHERS  \n",
       "453           what 's nature 's purpose for tornadoes ?         0  \n",
       "290     what is the history of valentine 's day cards ?         0  \n",
       "4457  what president became chief justice after his ...         3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec337132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>label-fine</th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>how far is it from denver to aspen ?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>what county is modesto , california in ?</td>\n",
       "      <td>OTHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>who was galileo ?</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>what is an atom ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>when did hawaii become a state ?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label-coarse  label-fine                                      text  \\\n",
       "0             4          40      how far is it from denver to aspen ?   \n",
       "1             5          21  what county is modesto , california in ?   \n",
       "2             3          12                         who was galileo ?   \n",
       "3             0           7                         what is an atom ?   \n",
       "4             4           8          when did hawaii become a state ?   \n",
       "\n",
       "  new_label  \n",
       "0         4  \n",
       "1    OTHERS  \n",
       "2         3  \n",
       "3         0  \n",
       "4         4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d8ca378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "test_data['new_label'] = test_data['new_label'].astype(str)\n",
    "test_data[\"label_transformed\"] = label_encoder.fit_transform(test_data['new_label'])\n",
    "\n",
    "train_data['new_label'] = train_data['new_label'].astype(str)\n",
    "train_data[\"label_transformed\"] = label_encoder.fit_transform(train_data['new_label'])\n",
    "\n",
    "dev_data['new_label'] = dev_data['new_label'].astype(str)\n",
    "dev_data[\"label_transformed\"] = label_encoder.fit_transform(dev_data['new_label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20426d8d",
   "metadata": {},
   "source": [
    "## Add \\<UNK> && \\<PAD>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5f00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.get(word, vocab.get('<UNK>')) for word in sentence]\n",
    "\n",
    "\n",
    "class TRECDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, vocab):\n",
    "        self.sentences = [torch.tensor(sentence_to_indices(sentence, vocab)) for sentence in sentences]\n",
    "        self.tags = [torch.tensor(tag) for tag in tags]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    return sentences_padded, tags\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TRECDataset(train_data['text'], train_data['label_transformed'], word2idx)\n",
    "dev_dataset = TRECDataset(dev_data['text'], dev_data['label_transformed'], word2idx)\n",
    "test_dataset = TRECDataset(test_data['text'], test_data['label_transformed'], word2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041513e5",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e633d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(w2v.vectors)\n",
    "\n",
    "# Build nn.Embedding() layer\n",
    "embedding = nn.Embedding.from_pretrained(weights)\n",
    "# embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=vocab.get('<PAD>', None), freeze=True)\n",
    "embedding.requires_grad = False\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(w2v.vectors)\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(QuestionClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix,  padding_idx=word2idx['<PAD>'], freeze=True)\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: batch_size x seq_length\n",
    "        x = self.embedding(x)    # Now, shape: batch_size x seq_length x embedding_dim\n",
    "        x = self.hidden(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Aggregation Layer: Averaging word vectors across sequence length\n",
    "        x = torch.mean(x, dim=1)  # Now, shape: batch_size x hidden_dim\n",
    "\n",
    "        # Output Layer\n",
    "        x = self.output(x)  # Now, shape: batch_size x output_dim\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34effcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "# OUTPUT_DIM = len(label_list)  # Number of unique tags/labels\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TAGSET_SIZE = 5\n",
    "\n",
    "model = QuestionClassifier(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1975aa0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 201.4536\n",
      "Epoch 2/100, Loss: 200.8260\n",
      "Epoch 3/100, Loss: 201.1263\n",
      "Epoch 4/100, Loss: 200.2615\n",
      "Epoch 5/100, Loss: 200.0161\n",
      "Epoch 6/100, Loss: 199.8823\n",
      "Epoch 7/100, Loss: 198.8634\n",
      "Epoch 8/100, Loss: 198.8925\n",
      "Epoch 9/100, Loss: 199.0258\n",
      "Epoch 10/100, Loss: 199.2024\n",
      "Epoch 11/100, Loss: 198.4924\n",
      "Epoch 12/100, Loss: 198.5416\n",
      "Epoch 13/100, Loss: 197.8683\n",
      "Epoch 14/100, Loss: 197.3834\n",
      "Epoch 15/100, Loss: 197.7136\n",
      "Epoch 16/100, Loss: 197.9380\n",
      "Epoch 17/100, Loss: 197.2971\n",
      "Epoch 18/100, Loss: 197.2957\n",
      "Epoch 19/100, Loss: 196.6785\n",
      "Epoch 20/100, Loss: 197.1304\n",
      "Epoch 21/100, Loss: 197.1357\n",
      "Epoch 22/100, Loss: 197.6837\n",
      "Epoch 23/100, Loss: 196.4490\n",
      "Epoch 24/100, Loss: 196.2947\n",
      "Epoch 25/100, Loss: 196.4248\n",
      "Epoch 26/100, Loss: 196.3772\n",
      "Epoch 27/100, Loss: 196.7078\n",
      "Epoch 28/100, Loss: 196.7052\n",
      "Epoch 29/100, Loss: 196.3514\n",
      "Epoch 30/100, Loss: 196.8837\n",
      "Epoch 31/100, Loss: 196.0057\n",
      "Epoch 32/100, Loss: 195.6482\n",
      "Epoch 33/100, Loss: 195.7314\n",
      "Epoch 34/100, Loss: 195.8251\n",
      "Epoch 35/100, Loss: 195.1844\n",
      "Epoch 36/100, Loss: 195.3353\n",
      "Epoch 37/100, Loss: 196.0006\n",
      "Epoch 38/100, Loss: 196.1507\n",
      "Epoch 39/100, Loss: 195.2039\n",
      "Epoch 40/100, Loss: 195.4513\n",
      "Epoch 41/100, Loss: 195.0984\n",
      "Epoch 42/100, Loss: 194.0551\n",
      "Epoch 43/100, Loss: 194.5252\n",
      "Epoch 44/100, Loss: 194.5863\n",
      "Epoch 45/100, Loss: 194.9994\n",
      "Epoch 46/100, Loss: 194.6474\n",
      "Epoch 47/100, Loss: 194.4197\n",
      "Epoch 48/100, Loss: 195.0060\n",
      "Epoch 49/100, Loss: 194.4383\n",
      "Epoch 50/100, Loss: 195.7332\n",
      "Epoch 51/100, Loss: 194.2500\n",
      "Epoch 52/100, Loss: 195.0108\n",
      "Epoch 53/100, Loss: 194.9529\n",
      "Epoch 54/100, Loss: 194.7586\n",
      "Epoch 55/100, Loss: 194.3269\n",
      "Epoch 56/100, Loss: 194.2321\n",
      "Epoch 57/100, Loss: 194.2701\n",
      "Epoch 58/100, Loss: 194.6164\n",
      "Epoch 59/100, Loss: 194.2230\n",
      "Epoch 60/100, Loss: 194.7664\n",
      "Epoch 61/100, Loss: 194.1477\n",
      "Epoch 62/100, Loss: 194.3116\n",
      "Epoch 63/100, Loss: 194.3013\n",
      "Epoch 64/100, Loss: 194.2215\n",
      "Epoch 65/100, Loss: 194.9551\n",
      "Epoch 66/100, Loss: 193.1266\n",
      "Epoch 67/100, Loss: 194.3220\n",
      "Epoch 68/100, Loss: 194.0763\n",
      "Epoch 69/100, Loss: 193.8024\n",
      "Epoch 70/100, Loss: 194.2990\n",
      "Epoch 71/100, Loss: 193.9900\n",
      "Epoch 72/100, Loss: 194.6529\n",
      "Epoch 73/100, Loss: 194.4985\n",
      "Epoch 74/100, Loss: 194.0185\n",
      "Epoch 75/100, Loss: 193.9630\n",
      "Epoch 76/100, Loss: 194.5388\n",
      "Epoch 77/100, Loss: 193.6120\n",
      "Epoch 78/100, Loss: 193.6621\n",
      "Epoch 79/100, Loss: 193.5626\n",
      "Epoch 80/100, Loss: 193.3171\n",
      "Epoch 81/100, Loss: 192.6223\n",
      "Epoch 82/100, Loss: 193.9404\n",
      "Epoch 83/100, Loss: 193.7943\n",
      "Epoch 84/100, Loss: 193.7847\n",
      "Epoch 85/100, Loss: 193.5992\n",
      "Epoch 86/100, Loss: 193.7921\n",
      "Epoch 87/100, Loss: 193.6631\n",
      "Epoch 88/100, Loss: 193.7861\n",
      "Epoch 89/100, Loss: 193.8193\n",
      "Epoch 90/100, Loss: 193.6090\n",
      "Epoch 91/100, Loss: 194.1634\n",
      "Epoch 92/100, Loss: 193.5881\n",
      "Epoch 93/100, Loss: 193.9291\n",
      "Epoch 94/100, Loss: 193.3169\n",
      "Epoch 95/100, Loss: 193.5996\n",
      "Epoch 96/100, Loss: 194.3440\n",
      "Epoch 97/100, Loss: 193.7344\n",
      "Epoch 98/100, Loss: 193.4782\n",
      "Epoch 99/100, Loss: 192.6641\n",
      "Epoch 100/100, Loss: 192.7628\n"
     ]
    }
   ],
   "source": [
    "# Assuming you've created dataloaders for training and validation data\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    for sentences, tag_tuple in train_loader:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Convert tuple of tensors to a single tensor\n",
    "        tags = torch.stack(tag_tuple)\n",
    "        \n",
    "        # Pass inputs through the model\n",
    "        predictions = model(sentences)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(predictions, tags)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "#     if (epoch+1) % 2 == 0:\n",
    "#         print(\"evaluate.....\")\n",
    "#         # Evaluate on the validation dataset\n",
    "#         # Placeholder to store true and predicted tags\n",
    "#         y_true = [] # true tags\n",
    "#         y_pred = [] # predicted tags\n",
    "        \n",
    "#         # Evaluate the model on the validation dataset\n",
    "#         model.eval()  # Set the model to evaluation mode\n",
    "#         with torch.no_grad():\n",
    "#             for sentences, tags in validation_loader:\n",
    "#                 tag_scores = model(sentences)\n",
    "#                 predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "                \n",
    "#                 # Convert index to tags\n",
    "#                 tag_seqs = [idx_to_tags(seq, {v: k for k, v in tag2idx.items()}) for seq in tags.tolist()]\n",
    "#                 pred_seqs = [idx_to_tags(seq, {v: k for k, v in tag2idx.items()}) for seq in predictions]\n",
    "                \n",
    "#                 y_true.extend(tag_seqs)\n",
    "#                 y_pred.extend(pred_seqs)\n",
    "        \n",
    "#         # Compute F1 score\n",
    "#         f1 = f1_score(y_true, y_pred)\n",
    "#         #report = classification_report(y_true, y_pred, mode='strict', scheme=IOB1)\n",
    "#         print(\"F1 Score:\", f1)\n",
    "#         #print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2e29237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 44.80%\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize counters\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Disable gradient computation (not needed during evaluation)\n",
    "with torch.no_grad():\n",
    "    for sentences, tag_tuple in test_loader:\n",
    "        # Convert tuple of tensors to a single tensor (like before)\n",
    "        tags = torch.stack(tag_tuple)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model(sentences)\n",
    "        \n",
    "        # Get the predicted class labels\n",
    "        _, predicted_labels = torch.max(predictions, 1)\n",
    "        \n",
    "        # Update counters\n",
    "        correct_predictions += (predicted_labels == tags).sum().item()\n",
    "        total_predictions += tags.size(0)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665557c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561173d6",
   "metadata": {},
   "source": [
    "# **Below Code is still Bull-shit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bbdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d849d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c58a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d71e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b0e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9354ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569bf17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd969c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd9495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe7442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722459f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence, model):\n",
    "    embeddings = []\n",
    "    for word in sentence.split():\n",
    "        if word in model:\n",
    "            embeddings.append(model[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(300))\n",
    "    return embeddings\n",
    "\n",
    "# def embedding(df, model):\n",
    "#     embeddings_list = []\n",
    "#     new_labels_list = []\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         sentence = row['text']\n",
    "#         embeddings = get_sentence_embeddings(sentence, model)\n",
    "        \n",
    "#         # Appending the embeddings and the corresponding new_label to the lists\n",
    "#         embeddings_list.append(embeddings)\n",
    "#         new_labels_list.append(row['new_label'])\n",
    "    \n",
    "#     # Creating the new dataframes\n",
    "#     X_train = pd.DataFrame({'embeddings': embeddings_list})\n",
    "#     Y_train = pd.DataFrame({'new_label': new_labels_list})\n",
    "    \n",
    "#     return X_train, Y_train\n",
    "\n",
    "def embedding(df, model):\n",
    "    embeddings_list = []\n",
    "    new_labels_list = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['text']\n",
    "        embeddings = get_sentence_embeddings(sentence, model)\n",
    "        \n",
    "        # Appending the embeddings and the corresponding new_label to the lists\n",
    "        embeddings_list.append(embeddings)\n",
    "        new_labels_list.append(row['new_label'])\n",
    "    \n",
    "    return embeddings_list, new_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6b4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = embedding(train_data, word2idx)\n",
    "X_dev, Y_dev = embedding(dev_data, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d19e2b",
   "metadata": {},
   "source": [
    "\n",
    "## Use a simple linear layer + Averaging all word vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57daa033",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# # Convert object-type DataFrame to list of arrays\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# X_train_arrays = list(X_train.apply(lambda x: np.array(x)))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Stack arrays\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m X_train_stacked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensor\u001b[39;00m\n\u001b[1;32m     10\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train_stacked, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/numpy/core/shape_base.py:286\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_vhstack_dispatcher)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvstack\u001b[39m(tup, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Stack arrays in sequence vertically (row wise).\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m \u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m         arrs \u001b[38;5;241m=\u001b[39m [arrs]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/openai/lib/python3.9/site-packages/numpy/core/shape_base.py:121\u001b[0m, in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    119\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[0;32m--> 121\u001b[0m     ary \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    123\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# # Convert object-type DataFrame to list of arrays\n",
    "# X_train_arrays = list(X_train.apply(lambda x: np.array(x)))\n",
    "\n",
    "# Stack arrays\n",
    "X_train_stacked = np.vstack(X_train)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_train_tensor = torch.tensor(X_train_stacked, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# # Convert object-type DataFrame to list of arrays\n",
    "# X_test_arrays = list(X_test.apply(lambda x: np.array(x)))\n",
    "\n",
    "# Stack arrays\n",
    "X_dev_stacked = np.vstack(X_dev)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "X_dev_tensor = torch.tensor(X_dev_stacked, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35767d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data['new_label'])\n",
    "y_dev = label_encoder.transform(dev_data['new_label'])\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_dev_tensor = torch.tensor(y_dev, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0687cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2711485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model architecture\n",
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(QuestionClassifier, self).__init__()\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: batch_size x seq_length x embedding_dim\n",
    "        x = self.hidden(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Aggregation Layer: Averaging word vectors across sequence length\n",
    "        x = torch.mean(x, dim=1)\n",
    "        \n",
    "        # Output Layer\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Define Hyperparameters\n",
    "EMBEDDING_DIM = 300  # As each word vector is of shape (300,)\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 5  # 4 integer classes + 1 \"OTHERS\"\n",
    "EPOCHS = 100\n",
    "LR = 0.001\n",
    "\n",
    "# # Convert dataframes to tensors\n",
    "# X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "# Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "# Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "# Instantiate the model\n",
    "model = QuestionClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = criterion(predictions, Y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f\"Epoch: {epoch}/{EPOCHS}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_tensor)\n",
    "    _, predicted = test_predictions.max(1)\n",
    "    accuracy = (predicted == Y_test_tensor).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9972697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489f12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d3f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c953c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d73867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(QuestionClassifier, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.mean(x, dim=1)  # Aggregation Layer\n",
    "        x = self.output(x)\n",
    "        return nn.functional.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 300  # word2vec dimensions\n",
    "hidden_dim = 128\n",
    "output_dim = 5  # 5 classes\n",
    "lr = 0.01\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = QuestionClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training\n",
    "epochs = 1000\n",
    "\n",
    "train_data = pd.DataFrame()  # Assuming train_data is already loaded as mentioned\n",
    "dev_data = pd.DataFrame()    # Assuming dev_data is also loaded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1629057",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for index, row in train_data.iterrows():\n",
    "        optimizer.zero_grad()\n",
    "        sentence = row['text']\n",
    "        label = row['new_label']\n",
    "\n",
    "        sentence_embeddings = torch.tensor(get_sentence_embeddings(sentence, glove_vectors)).float().unsqueeze(0)\n",
    "        \n",
    "        outputs = model(sentence_embeddings)\n",
    "        loss = criterion(outputs, torch.tensor([label]).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for index, row in dev_data.iterrows():\n",
    "        sentence = row['text']\n",
    "        label = row['new_label']\n",
    "        true_labels.append(label)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sentence_embedding = torch.tensor(get_sentence_embedding(sentence, glove_vectors)).float().unsqueeze(0)\n",
    "            outputs = model(sentence_embedding)\n",
    "            predicted_class = torch.argmax(outputs).item()\n",
    "            predictions.append(predicted_class)\n",
    "            \n",
    "    dev_accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {np.mean(train_losses)}, Dev Accuracy: {dev_accuracy:.2f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda366d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f5e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38660c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa35c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the average word vector for a sentence\n",
    "def sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    # Split the sentence into words.\n",
    "\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    # Get the word vector for each word in the sentence if it exists in glove_vectors.\n",
    "\n",
    "\n",
    "    if len(vectors) == 0: # to avoid empty lists\n",
    "        return np.zeros(300)\n",
    "    # If no words from the sentence are in the word vectors, return a vector of zeros.\n",
    "\n",
    "    return np.mean(vectors, axis=0)\n",
    "# Return the average word vector for the sentence.\n",
    "\n",
    "train_data['avg_vector'] = train_data['text'].apply(sentence_vector)\n",
    "# Compute the average word vector for each sentence in the training data.\n",
    "\n",
    "X_train = np.vstack(train_data['avg_vector'].values)\n",
    "# Stack the average vectors to form the training data.\n",
    "\n",
    "y_train = pd.get_dummies(train_data['new_label']).values\n",
    "# Convert the new labels to one-hot encoded vectors.\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# Convert the training data to a PyTorch tensor.\n",
    "\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.int64)\n",
    "# Convert the one-hot encoded labels to their corresponding class indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2d981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network Model with a simple linear layer\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        # Initialize the parent class.\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        # Define a fully connected layer.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    # Define the forward pass to return the output of the linear layer.\n",
    "\n",
    "input_dim = 300  # as we're using word2vec-google-news-300\n",
    "# Define the input dimension based on the word vector size.\n",
    "\n",
    "output_dim = 5  # for our 5 new classes\n",
    "# Define the output dimension based on the number of new classes.\n",
    "\n",
    "model = SimpleClassifier(input_dim, output_dim)\n",
    "# Initialize the model.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the loss function (cross entropy).\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define the optimizer (Adam) with a learning rate.\n",
    "\n",
    "# Training\n",
    "epochs = 5000\n",
    "# Set the number of epochs.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "  \n",
    "\n",
    "\n",
    "    # Store metrics for visualization\n",
    "    all_metrics['A']['epochs'].append(epoch)\n",
    "    all_metrics['A']['accuracy'].append(accuracy)\n",
    "    all_metrics['A']['loss'].append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "    # Print the loss and accuracy for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(all_metrics['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df5364",
   "metadata": {},
   "source": [
    "## Use a feedforward network which is a combination of a linear transformation and a nonlinear activation function\n",
    "\n",
    "## Max pooling over the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    if len(vectors) == 0: # if no words in the sentence have embeddings\n",
    "        return np.zeros(300)\n",
    "    return np.max(vectors, axis=0) # max pooling across the words\n",
    "\n",
    "train_data['maxpooled_vector'] = train_data['text'].apply(sentence_vector)\n",
    "\n",
    "X_train = np.vstack(train_data['maxpooled_vector'].values)\n",
    "y_train = pd.get_dummies(train_data['new_label']).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08402d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7b651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network model with feedforward layers\n",
    "class FeedForwardClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedForwardClassifier, self).__init__()\n",
    "        \n",
    "        # First linear layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Second linear layer that outputs class probabilities\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply first linear transformation\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Apply ReLU activation function\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        # Apply second linear transformation\n",
    "        return self.fc2(x)\n",
    "\n",
    "input_dim = 300  # as we're using word2vec-google-news-300\n",
    "hidden_dim = 1000  # can be adjusted based on performance\n",
    "output_dim = 5   # for our 5 new classes\n",
    "\n",
    "model = FeedForwardClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "\n",
    "    # Store metrics for visualization\n",
    "    all_metrics['B']['epochs'].append(epoch)\n",
    "    all_metrics['B']['accuracy'].append(accuracy)\n",
    "    all_metrics['B']['loss'].append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(all_metrics['B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632f1bb",
   "metadata": {},
   "source": [
    "##  Recurrent neural network\n",
    "##  Aggregation Layer: Taking the representation of the last word (useful if using RNNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_matrix(sentence, max_len=30):\n",
    "    words = sentence.split()[:max_len]  # truncate if necessary\n",
    "    vectors = [glove_vectors[word] for word in words if word in glove_vectors]\n",
    "    while len(vectors) < max_len:  # pad if necessary\n",
    "        vectors.append(np.zeros(300))\n",
    "    return np.array(vectors)\n",
    "\n",
    "train_data['vector_matrix'] = train_data['text'].apply(sentence_matrix)\n",
    "\n",
    "X_train = np.stack(train_data['vector_matrix'].values)\n",
    "y_train = pd.get_dummies(train_data['new_label']).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f442c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network model with an RNN layer\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Linear layer that outputs class probabilities\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the RNN layer\n",
    "        out, _ = self.rnn(x)\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass the final output through the linear layer\n",
    "        return self.fc(out)\n",
    "\n",
    "input_dim = 300  # as we're using word2vec-google-news-300\n",
    "hidden_dim = 100  # can be adjusted based on performance\n",
    "output_dim = 5   # for our 5 new classes\n",
    "\n",
    "model = RNNClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_train_tensor).float().mean().item()\n",
    "\n",
    "    # Store metrics for visualization\n",
    "    all_metrics['C']['epochs'].append(epoch)\n",
    "    all_metrics['C']['accuracy'].append(accuracy)\n",
    "    all_metrics['C']['loss'].append(loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_metrics(all_metrics['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77760679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4731a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f42caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d856afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
