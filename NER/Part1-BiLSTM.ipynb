{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f23042a-5f2e-483f-b888-8ff2fcb9cfe3",
   "metadata": {},
   "source": [
    "# 1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6eb528a-a228-4d55-b62d-bd22b3a9c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/word2vec-google-news-300.vectors.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/gensim/utils.py:764\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Download the model using gensim.downloader and save it to the directory\u001b[39;00m\n\u001b[1;32m     15\u001b[0m w2v \u001b[38;5;241m=\u001b[39m gensim\u001b[38;5;241m.\u001b[39mdownloader\u001b[38;5;241m.\u001b[39mload(model_name)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/gensim/models/keyedvectors.py:774\u001b[0m, in \u001b[0;36mKeyedVectors.save\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save KeyedVectors to a file.\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    772\u001b[0m \n\u001b[1;32m    773\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKeyedVectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/gensim/utils.py:767\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    765\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/gensim/utils.py:607\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save the object to a file. Used internally by :meth:`gensim.utils.SaveLoad.save()`.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m \n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    605\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[0;32m--> 607\u001b[0m restores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_specials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     pickle(\u001b[38;5;28mself\u001b[39m, fname, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/gensim/utils.py:683\u001b[0m, in \u001b[0;36mSaveLoad._save_specials\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)\u001b[0m\n\u001b[1;32m    681\u001b[0m         np\u001b[38;5;241m.\u001b[39msavez_compressed(subname(fname, attrib), val\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mascontiguousarray(val))\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 683\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrib\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsc_matrix)) \u001b[38;5;129;01mand\u001b[39;00m attrib \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore:\n\u001b[1;32m    686\u001b[0m     scipys\u001b[38;5;241m.\u001b[39mappend(attrib)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/numpy/lib/npyio.py:518\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    517\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/word2vec-google-news-300.vectors.npy'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import os\n",
    "model_name = 'word2vec-google-news-300'\n",
    "\n",
    "# Define the full path for the model\n",
    "model_path = os.path.join('models', model_name)\n",
    "# Check if the model is already in the specified directory\n",
    "if os.path.isfile(model_path):\n",
    "    # Load the model from the specified directory\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    w2v = gensim.models.KeyedVectors.load(model_path)\n",
    "else:\n",
    "    print(f\"Downloading...\")\n",
    "    # Download the model using gensim.downloader and save it to the directory\n",
    "    w2v = gensim.downloader.load(model_name)\n",
    "    w2v.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cd298-8b17-4757-ac41-fdf6d4fb8915",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "use cosine similarity to find the most similar \n",
    "word to each of these worsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab65046-7971-4d01-abd7-28135a3bb217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tMost similar word\tCosine similarity\n",
      "=======================================================================\n",
      "student        \tstudents       \t\t0.7295\n",
      "Apple          \tApple_AAPL     \t\t0.7457\n",
      "apple          \tapples         \t\t0.7204\n"
     ]
    }
   ],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "\n",
    "# Print the header\n",
    "print(\"Word\\t\\tMost similar word\\tCosine similarity\")\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "for word in words:\n",
    "    # Use the downloaded vectors as usual:\n",
    "    most_similar = w2v.most_similar(positive=[word], topn=1)[0]\n",
    "    print(\"{:<15}\\t{:<15}\\t\\t{:.4f}\".format(word, most_similar[0], most_similar[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed0dd1-ea8d-4de5-bffc-86bc0ecc51ee",
   "metadata": {},
   "source": [
    "# 1.2 Data\n",
    "process: https://wandb.ai/mostafaibrahim17/ml-articles/reports/Named-Entity-Recognition-With-HuggingFace-Using-PyTorch-and-W-B--Vmlldzo0NDgzODA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79fcdfa-f1da-4c96-a145-298ab4952340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephanie/GitHub/SC4002_G06/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_conll_file(\"../datasets/CoNLL2003/eng.train\")\n",
    "validation_data = read_conll_file(\"../datasets/CoNLL2003/eng.testa\")\n",
    "test_data = read_conll_file(\"../datasets/CoNLL2003/eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5137c1c0-eecc-4ee5-a7b7-a0101c7a4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def convert_to_dataset(data, label_map):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "\n",
    "label_list = sorted(list(set([token_data[3] for sentence in train_data for token_data in sentence])))\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "train_dataset = convert_to_dataset(train_data, label_map)\n",
    "validation_dataset = convert_to_dataset(validation_data, label_map)\n",
    "test_dataset = convert_to_dataset(test_data, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d226892-7cd6-44c6-abf1-d770852968cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-MISC': 1,\n",
       " 'B-ORG': 2,\n",
       " 'I-LOC': 3,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 5,\n",
       " 'I-PER': 6,\n",
       " 'O': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7418a-e9e0-4f0c-91c8-b8d5193aedb3",
   "metadata": {},
   "source": [
    "## Question 1.2\n",
    "(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
    "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
    "etc.) you chos\n",
    "\n",
    "(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
    "entities with more than one word. Explain how to form complete named entities from the label\n",
    "for each word, and list all the named entities in this sentence.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed55a4ea-ffcb-49c5-bc67-1f90ab083da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a)\n",
    "print(\"Dataset Sizes:\")\n",
    "print(f\"Training:\\t{train_dataset.num_rows} sentences\")\n",
    "print(f\"Development:\\t{validation_dataset.num_rows} sentences\")\n",
    "print(f\"Test:\\t\\t{test_dataset.num_rows} sentences\")\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "print(\"All Possible Word Labels (BIO):\\n\", label_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf2505-7f3a-4fd2-a736-a360b94dae30",
   "metadata": {},
   "source": [
    "!! (b) means finding the sentence that contains at least two distinct named entities, and each of those entities consists of more than one word.\n",
    "=> but seems in training dataset, there isn't this kind of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8cd84b-8caf-4486-ab9e-e000ef503aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "lists2 = [0,1,2] # ['B-LOC', 'B-MISC', 'B-ORG']\n",
    "\n",
    "def has_at_least_two_common_elements(list1, list2=[0,1,2]):\n",
    "    common_elements = [value for value in list1 if value in list2]\n",
    "    return len(common_elements) >= 2\n",
    "\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    tokens = train_dataset['tokens'][i]\n",
    "    ner_tags = train_dataset['ner_tags'][i]\n",
    "\n",
    "    if has_at_least_two_common_elements(ner_tags):\n",
    "        print(i)\n",
    "        print(tokens)\n",
    "        print(ner_tags)\n",
    "        print([label_list[tag] for tag in ner_tags])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f82114d-a819-4908-8143-0087bcb9938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [label_list[tag] for tag in train_dataset[5969]['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27fe1611-8fe9-45aa-a8df-ad2e8d8a7988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b)\n",
    "def form_complete_ne(dataset, i):\n",
    "    # define sets of tags\n",
    "    begin_tags = {'B-LOC', 'B-ORG', 'B-MISC'}\n",
    "    inside_tags = {'I-ORG', 'I-LOC', 'I-PER', 'I-MISC'}\n",
    "    outside_tags = {'O'}\n",
    "\n",
    "    words = []\n",
    "    word = []\n",
    "    entities = []\n",
    "    entity = []\n",
    "\n",
    "    tokens = dataset['tokens'][i]\n",
    "    ner_tags = dataset['ner_tags'][i]\n",
    "\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        tag = label_list[tag]\n",
    "\n",
    "        if (tag in begin_tags or tag in outside_tags) and word:\n",
    "            words.append(' '.join(word))\n",
    "            entities.append(' '.join(entity))\n",
    "            word = []\n",
    "            entity = []\n",
    "\n",
    "        if tag in begin_tags or tag in inside_tags:\n",
    "            word.append(token)\n",
    "            entity.append(tag)\n",
    "\n",
    "    if word:\n",
    "        words.append(' '.join(word))\n",
    "        entities.append(' '.join(entity))\n",
    "\n",
    "    return words, entities\n",
    "\n",
    "form_complete_ne(train_dataset, 5969)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7891e6-272f-49fa-9d63-19049f999a10",
   "metadata": {},
   "source": [
    "# 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db447bac-3109-4c13-8257-3bca386dde26",
   "metadata": {},
   "source": [
    "1. `<PAD>` Token:\n",
    "\n",
    "This token is typically initialized to a zero vector because it's meant to be a neutral padding value that doesn't interfere with computation\n",
    "\n",
    "2. `<UNK>` Token:\n",
    "- Zero Vector: Similar to the <PAD> token, you can initialize it to a zero vector.\n",
    "- Average Vector: Initialize it as the average of all word vectors in your pretrained embeddings. This gives it a kind of \"average\" representation of the language.\n",
    "- Random Vector: Randomly initialize it, which might add some noise and robustness to the embeddi\n",
    "\n",
    "For many tasks, initializing the <UNK> token as the average of all word vectors works well. It makes the <UNK> token have a representation that is, on average, similar to any random word from the vocabulary, which can be beneficial since the <UNK> token is used for words that aren't in the training vocabulary but could be anywhere in the semantic space.ngs.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "606d6017-bbc5-476a-bc69-1356030f91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16c36319-becd-4f14-be61-614cc35852e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether <UNK> in w2v: False\n",
      "whether <PAD> in w2v: False\n",
      "word2idx['<UNK>']: 3000000\n",
      "word2idx['<PAD>']: 3000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after insert UNK:  (3000001, 300)\n",
      "after insert UNK:  (3000002, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Out-of-vocabulary (OOV) words\n",
    "# 1. can be replaced with a special token, such as \"<OOV>\" or \"<UNK>\".\n",
    "# 2. can be ignored.\n",
    "\n",
    "word2idx = w2v.key_to_index\n",
    "print(f\"whether <UNK> in w2v: {'<UNK>' in word2idx}\") # False\n",
    "print(f\"whether <PAD> in w2v: {'<PAD>' in word2idx}\") # False\n",
    "\n",
    "# Add '<UNK>' and '<PAD>' tokens to the vocabulary index\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "print(f\"word2idx['<UNK>']: {word2idx['<UNK>']}\")\n",
    "print(f\"word2idx['<PAD>']: {word2idx['<PAD>']}\")\n",
    "\n",
    "# add the '<UNK>' word to the vocabulary of the Word2Vec model\n",
    "# initialize it with the average of all word vectors in the pretrained embeddings.\n",
    "unk_vector = np.mean(w2v.vectors, axis=0)\n",
    "w2v.vectors = np.vstack([w2v.vectors, unk_vector])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n",
    "\n",
    "# add the '<PAD>' word to the vocabulary of the Word2Vec model\n",
    "# initialize it with a row of zeros in the vectors matrix.\n",
    "w2v.vectors = np.vstack([w2v.vectors, np.zeros(w2v.vectors[0].shape)])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fbbae-6788-4067-85c8-23ed010199e6",
   "metadata": {},
   "source": [
    "# Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3beb52ff-de17-4f67-899c-6b7e8dc58802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to Indices\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.get(word, vocab.get('<UNK>')) for word in sentence]\n",
    "\n",
    "tag2idx = {\n",
    "    'B-LOC': 0,\n",
    "    'B-MISC': 1,\n",
    "    'B-ORG': 2,\n",
    "    'I-LOC': 3,\n",
    "    'I-MISC': 4,\n",
    "    'I-ORG': 5,\n",
    "    'I-PER': 6,\n",
    "    'O': 7,\n",
    "    'PAD': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55757fa-5020-481d-b1c1-ba3faa21062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, vocab):\n",
    "        self.sentences = [torch.tensor(sentence_to_indices(sentence, vocab)) for sentence in sentences]\n",
    "        self.tags = [torch.tensor(tag) for tag in tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = NERDataset(train_dataset['tokens'], train_dataset['ner_tags'], word2idx)\n",
    "validation_dataset = NERDataset(validation_dataset['tokens'], validation_dataset['ner_tags'], word2idx)\n",
    "test_dataset = NERDataset(test_dataset['tokens'], test_dataset['ner_tags'], word2idx)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=tag2idx['PAD'])\n",
    "    return sentences_padded, tags_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d01a3-0621-413d-b675-ae79a6ff8866",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cee6066-7544-4b7b-97b5-418a7788f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(w2v.vectors)\n",
    "\n",
    "class LSTMNERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMNERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=word2idx['<PAD>'], freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        tag_scores = torch.log_softmax(tag_space, dim=-1)\n",
    "        return tag_scores\n",
    "\n",
    "class BiLSTMNERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_of_layers, output_dim):\n",
    "        super(BiLSTMNERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=word2idx['<PAD>'], freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_of_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        tag_scores = torch.log_softmax(tag_space, dim=-1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0cb07-a595-46c5-bf81-9331df1ba55c",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a66728d-97bf-4e65-a9e6-304671240573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM: 300\n",
      "VOCAB_SIZE: 3000002\n",
      "TAGSET_SIZE: 9\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = w2v[0].shape[0]\n",
    "print(f\"EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "HIDDEN_DIM = 150\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "print(f\"TAGSET_SIZE: {TAGSET_SIZE}\")\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def early_stop(self, f1):\n",
    "        if f1 > self.max_f1:\n",
    "            self.max_f1 = f1\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d1f82-b9ee-4532-8a5b-37b917992ec7",
   "metadata": {},
   "source": [
    "# Train & Test\n",
    "https://necromuralist.github.io/Neurotic-Networking/posts/nlp/ner-evaluating-the-model/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "417a7386-2f72-4037-baca-265d17c621aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "def idx_to_tags(indices):\n",
    "    return [idx2tag[idx] for idx in indices]\n",
    "\n",
    "def test(model, batch_size):\n",
    "    # Placeholder to store true and predicted tags for the test set\n",
    "    y_true_test = []\n",
    "    y_pred_test = []\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in test_loader:\n",
    "            tag_scores = model(sentences)\n",
    "            predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "\n",
    "            # Convert index to tags\n",
    "            # Note: filtering out padding tokens\n",
    "            for sentence, true_seq, pred_seq in zip(sentences, tags.tolist(), predictions):\n",
    "                valid_len = (sentence != word2idx['<PAD>']).sum().item()\n",
    "                true_tags = idx_to_tags(true_seq[:valid_len])\n",
    "                pred_tags = idx_to_tags(pred_seq[:valid_len])\n",
    "                y_true_test.append(true_tags)\n",
    "                y_pred_test.append(pred_tags)\n",
    "\n",
    "    # Compute F1 score for the test set\n",
    "    f1_test = f1_score(y_true_test, y_pred_test)\n",
    "    #report_test = classification_report(y_true_test, y_pred_test)\n",
    "\n",
    "    print(\"F1 Score on Test Set:\", f1_test)\n",
    "    return f1_test\n",
    "    #print(\"Classification Report on Test Set:\\n\", report_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "611036c7-a564-4fe9-8ed0-c34a2c058e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  16\n",
      "Epoch 1, Loss: 199.0441536065191\n",
      "F1 Score: 0.8257945182444929\n",
      "Epoch 2, Loss: 85.18818697798997\n",
      "F1 Score: 0.8537429318929868\n",
      "Epoch 3, Loss: 64.29607850406319\n",
      "F1 Score: 0.8659322033898305\n",
      "Epoch 4, Loss: 49.4091847599484\n",
      "F1 Score: 0.8773888043294436\n",
      "Epoch 5, Loss: 37.594765432178974\n",
      "F1 Score: 0.884465526003727\n",
      "Epoch 6, Loss: 28.330530517501757\n",
      "F1 Score: 0.8881350182342465\n",
      "Epoch 7, Loss: 20.579610534594394\n",
      "F1 Score: 0.8809140922556072\n",
      "Epoch 8, Loss: 13.929533022630494\n",
      "F1 Score: 0.888926544098958\n",
      "Epoch 9, Loss: 10.1325377827161\n",
      "F1 Score: 0.8877041550308877\n",
      "Epoch 10, Loss: 6.61751740440377\n",
      "F1 Score: 0.8836342062886685\n",
      "Epoch 11, Loss: 6.029687131769606\n",
      "F1 Score: 0.8846056729231546\n",
      "Epoch 12, Loss: 4.695050049253041\n",
      "F1 Score: 0.8828480053862986\n",
      "Epoch 13, Loss: 4.621472282116883\n",
      "F1 Score: 0.8882288042094543\n",
      "F1 Score on Test Set: 0.8262062859672421\n",
      "Batch Size:  32\n",
      "Epoch 1, Loss: 130.979077398777\n",
      "F1 Score: 0.79986517232662\n",
      "Epoch 2, Loss: 49.51226925291121\n",
      "F1 Score: 0.8406602661276739\n",
      "Epoch 3, Loss: 38.991529311053455\n",
      "F1 Score: 0.8642978003384094\n",
      "Epoch 4, Loss: 31.87956314906478\n",
      "F1 Score: 0.8744918699186991\n",
      "Epoch 5, Loss: 26.150762940291315\n",
      "F1 Score: 0.8737929866169746\n",
      "Epoch 6, Loss: 21.715935247950256\n",
      "F1 Score: 0.8826862894358455\n",
      "Epoch 7, Loss: 17.409341059159487\n",
      "F1 Score: 0.8750950088674944\n",
      "Epoch 8, Loss: 13.521909564267844\n",
      "F1 Score: 0.8790390796819488\n",
      "Epoch 9, Loss: 10.440667059272528\n",
      "F1 Score: 0.8805668016194332\n",
      "Epoch 10, Loss: 7.763573045376688\n",
      "F1 Score: 0.8803454991955288\n",
      "Epoch 11, Loss: 5.737324537243694\n",
      "F1 Score: 0.8809523809523809\n",
      "F1 Score on Test Set: 0.8162513162513162\n",
      "Batch Size:  64\n",
      "Epoch 1, Loss: 93.71162434667349\n",
      "F1 Score: 0.7533609537498942\n",
      "Epoch 2, Loss: 29.540432658046484\n",
      "F1 Score: 0.8155471060414027\n",
      "Epoch 3, Loss: 23.458882808685303\n",
      "F1 Score: 0.8257317903335603\n",
      "Epoch 4, Loss: 20.079733077436686\n",
      "F1 Score: 0.8456900212314226\n",
      "Epoch 5, Loss: 17.26056342944503\n",
      "F1 Score: 0.856611115810845\n",
      "Epoch 6, Loss: 14.973588202148676\n",
      "F1 Score: 0.867214642826879\n",
      "Epoch 7, Loss: 13.067266253754497\n",
      "F1 Score: 0.871478128437262\n",
      "Epoch 8, Loss: 11.28062815591693\n",
      "F1 Score: 0.868565632659961\n",
      "Epoch 9, Loss: 9.73178961314261\n",
      "F1 Score: 0.8674232870954182\n",
      "Epoch 10, Loss: 8.179275074973702\n",
      "F1 Score: 0.8737716028464928\n",
      "Epoch 11, Loss: 6.857154045253992\n",
      "F1 Score: 0.8679566959338506\n",
      "Epoch 12, Loss: 5.648263962939382\n",
      "F1 Score: 0.8664987405541562\n",
      "Epoch 13, Loss: 4.551194620784372\n",
      "F1 Score: 0.8720419202163623\n",
      "Epoch 14, Loss: 3.6421432544011623\n",
      "F1 Score: 0.863686301022564\n",
      "Epoch 15, Loss: 2.8803887895774096\n",
      "F1 Score: 0.8651742130954385\n",
      "F1 Score on Test Set: 0.8059832050384883\n",
      "Batch Size:  128\n",
      "Epoch 1, Loss: 72.57644057273865\n",
      "F1 Score: 0.621194111730822\n",
      "Epoch 2, Loss: 20.783474668860435\n",
      "F1 Score: 0.7698787061994609\n",
      "Epoch 3, Loss: 14.73375216126442\n",
      "F1 Score: 0.8034912295568172\n",
      "Epoch 4, Loss: 12.576788138598204\n",
      "F1 Score: 0.8217737832796337\n",
      "Epoch 5, Loss: 11.170249350368977\n",
      "F1 Score: 0.8363052480351558\n",
      "Epoch 6, Loss: 10.118934530764818\n",
      "F1 Score: 0.8414819827440366\n",
      "Epoch 7, Loss: 9.230666007846594\n",
      "F1 Score: 0.8492849284928492\n",
      "Epoch 8, Loss: 8.29425137117505\n",
      "F1 Score: 0.8519616981611728\n",
      "Epoch 9, Loss: 7.5253779627382755\n",
      "F1 Score: 0.8549682875264271\n",
      "Epoch 10, Loss: 6.922117758542299\n",
      "F1 Score: 0.8591153157760163\n",
      "Epoch 11, Loss: 6.267828356474638\n",
      "F1 Score: 0.8615201293727126\n",
      "Epoch 12, Loss: 5.781328285112977\n",
      "F1 Score: 0.8646140868831717\n",
      "Epoch 13, Loss: 5.1427189745008945\n",
      "F1 Score: 0.8684455739093604\n",
      "Epoch 14, Loss: 4.640686692669988\n",
      "F1 Score: 0.8644225090694339\n",
      "Epoch 15, Loss: 4.215489909052849\n",
      "F1 Score: 0.8670794808719993\n",
      "Epoch 16, Loss: 3.723979844711721\n",
      "F1 Score: 0.8628113127902068\n",
      "Epoch 17, Loss: 3.1960836024954915\n",
      "F1 Score: 0.8684165961049959\n",
      "Epoch 18, Loss: 2.8540447279810905\n",
      "F1 Score: 0.8630425621576064\n",
      "F1 Score on Test Set: 0.8052949943017446\n",
      "Batch Size:  256\n",
      "Epoch 1, Loss: 50.537375658750534\n",
      "F1 Score: 0.0\n",
      "Epoch 2, Loss: 21.38543789088726\n",
      "F1 Score: 0.6160855416085542\n",
      "Epoch 3, Loss: 11.0536522641778\n",
      "F1 Score: 0.7454388984509467\n",
      "Epoch 4, Loss: 8.322159118950367\n",
      "F1 Score: 0.7758766728781976\n",
      "Epoch 5, Loss: 7.2012399062514305\n",
      "F1 Score: 0.8018636171113935\n",
      "Epoch 6, Loss: 6.477610245347023\n",
      "F1 Score: 0.8100346313033195\n",
      "Epoch 7, Loss: 5.9058887884020805\n",
      "F1 Score: 0.8291980692692015\n",
      "Epoch 8, Loss: 5.517063103616238\n",
      "F1 Score: 0.8299643281807373\n",
      "Epoch 9, Loss: 5.133557498455048\n",
      "F1 Score: 0.8389114266396214\n",
      "Epoch 10, Loss: 4.801726654171944\n",
      "F1 Score: 0.8445991847826086\n",
      "Epoch 11, Loss: 4.458774048835039\n",
      "F1 Score: 0.8436969079170914\n",
      "Epoch 12, Loss: 4.213607836514711\n",
      "F1 Score: 0.8513708513708513\n",
      "Epoch 13, Loss: 4.001183677464724\n",
      "F1 Score: 0.8540393754243043\n",
      "Epoch 14, Loss: 3.7513933293521404\n",
      "F1 Score: 0.8462840697477568\n",
      "Epoch 15, Loss: 3.5926306061446667\n",
      "F1 Score: 0.853446088794926\n",
      "Epoch 16, Loss: 3.302833631634712\n",
      "F1 Score: 0.8522920203735144\n",
      "Epoch 17, Loss: 3.1058928668498993\n",
      "F1 Score: 0.8563461213452763\n",
      "Epoch 18, Loss: 2.9146465957164764\n",
      "F1 Score: 0.8621890124819563\n",
      "Epoch 19, Loss: 2.7016335483640432\n",
      "F1 Score: 0.8553277994240217\n",
      "Epoch 20, Loss: 2.582275016233325\n",
      "F1 Score: 0.8632862101075087\n",
      "Epoch 21, Loss: 2.388136800378561\n",
      "F1 Score: 0.8627118644067797\n",
      "Epoch 22, Loss: 2.204520979896188\n",
      "F1 Score: 0.8611676849966056\n",
      "Epoch 23, Loss: 2.044619819149375\n",
      "F1 Score: 0.8583936689678399\n",
      "Epoch 24, Loss: 1.90179611928761\n",
      "F1 Score: 0.8646680216802168\n",
      "Epoch 25, Loss: 1.7265224289149046\n",
      "F1 Score: 0.8588512716860368\n",
      "Epoch 26, Loss: 1.6130184344947338\n",
      "F1 Score: 0.8638663967611335\n",
      "Epoch 27, Loss: 1.4412163496017456\n",
      "F1 Score: 0.8609933887099508\n",
      "Epoch 28, Loss: 1.3555732537060976\n",
      "F1 Score: 0.8647138503217067\n",
      "Epoch 29, Loss: 1.2129557058215141\n",
      "F1 Score: 0.8676271186440678\n",
      "Epoch 30, Loss: 1.1201992826536298\n",
      "F1 Score: 0.8667907921462423\n",
      "Epoch 31, Loss: 1.0024230079725385\n",
      "F1 Score: 0.8618382041507835\n",
      "Epoch 32, Loss: 0.909700233489275\n",
      "F1 Score: 0.8644440683702826\n",
      "Epoch 33, Loss: 0.8032148857600987\n",
      "F1 Score: 0.8628314835211388\n",
      "Epoch 34, Loss: 0.7557929875329137\n",
      "F1 Score: 0.8623108782013356\n",
      "F1 Score on Test Set: 0.7999297074070819\n",
      "f1 of testset with different batch size:  [0.8262062859672421, 0.8162513162513162, 0.8059832050384883, 0.8052949943017446, 0.7999297074070819]\n",
      "f1 of trainset with different batch size:  [0.8882288042094543, 0.8809523809523809, 0.8651742130954385, 0.8630425621576064, 0.8623108782013356]\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_batch_size(parameters):\n",
    "    f1_score_batch = []\n",
    "    f1_score_test = []\n",
    "    for batch_size in parameters:\n",
    "        model = BiLSTMNERModel(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE)\n",
    "        loss_function = nn.NLLLoss(ignore_index=tag2idx['PAD'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(\"Batch Size: \", batch_size)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        early_stopper = EarlyStopper()\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for sentences, tags in train_loader:\n",
    "                tag_scores = model(sentences)\n",
    "                loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "            model.eval()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for sentences, tags in validation_loader:\n",
    "                    tag_scores = model(sentences)\n",
    "                    predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "                    # Convert index to tags and compute F1 score\n",
    "                    for sentence, true_seq, pred_seq in zip(sentences, tags.tolist(), predictions):\n",
    "                        valid_len = (sentence != word2idx['<PAD>']).sum().item()\n",
    "                        true_tags = [idx2tag[idx] for idx in true_seq[:valid_len]]\n",
    "                        pred_tags = [idx2tag[idx] for idx in pred_seq[:valid_len]]\n",
    "                        y_true.append(true_tags)\n",
    "                        y_pred.append(pred_tags)\n",
    "\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            print(\"F1 Score:\", f1)\n",
    "            if early_stopper.early_stop(f1):\n",
    "                f1_score_batch.append(f1)\n",
    "                break\n",
    "        f1_score_test.append(test(model, batch_size))\n",
    "    print(\"f1 of testset with different batch size: \", f1_score_test)\n",
    "    print(\"f1 of trainset with different batch size: \", f1_score_batch)\n",
    "    return f1_score_test\n",
    "\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "f1_score_batch = find_optimal_batch_size(batch_sizes)\n",
    "optimal_batch_size = batch_sizes[np.argmax(f1_score_batch)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7ab2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Dimension:  32\n",
      "Epoch 1, Loss: 293.0157448761165\n",
      "F1 Score: 0.7713503195285916\n",
      "Epoch 2, Loss: 113.53828140161932\n",
      "F1 Score: 0.8185937237791576\n",
      "Epoch 3, Loss: 92.73915387224406\n",
      "F1 Score: 0.8364674278038953\n",
      "Epoch 4, Loss: 77.6808013478294\n",
      "F1 Score: 0.8493335582925595\n",
      "Epoch 5, Loss: 68.0739809833467\n",
      "F1 Score: 0.8589754409654823\n",
      "Epoch 6, Loss: 59.714668814092875\n",
      "F1 Score: 0.863010225640159\n",
      "Epoch 7, Loss: 52.77886861003935\n",
      "F1 Score: 0.8632558139534884\n",
      "Epoch 8, Loss: 46.79307142365724\n",
      "F1 Score: 0.8727733220768256\n",
      "Epoch 9, Loss: 41.281550084240735\n",
      "F1 Score: 0.8693739424703891\n",
      "Epoch 10, Loss: 36.87571499473415\n",
      "F1 Score: 0.8758335443572213\n",
      "Epoch 11, Loss: 32.7561949826777\n",
      "F1 Score: 0.8726658217152513\n",
      "Epoch 12, Loss: 29.140497231506743\n",
      "F1 Score: 0.8734487125369355\n",
      "Epoch 13, Loss: 26.04674756480381\n",
      "F1 Score: 0.8753373819163293\n",
      "Epoch 14, Loss: 22.698843191028573\n",
      "F1 Score: 0.8769438810006761\n",
      "Epoch 15, Loss: 19.987431074492633\n",
      "F1 Score: 0.8735127837313307\n",
      "Epoch 16, Loss: 17.727546258829534\n",
      "F1 Score: 0.8606152040901853\n",
      "Epoch 17, Loss: 15.910955313884187\n",
      "F1 Score: 0.8745676929565583\n",
      "Epoch 18, Loss: 13.431694288155995\n",
      "F1 Score: 0.8741911084965122\n",
      "Epoch 19, Loss: 11.704875610303134\n",
      "F1 Score: 0.874460340303056\n",
      "F1 Score on Test Set: 0.8039890565704705\n",
      "Hidden Dimension:  64\n",
      "Epoch 1, Loss: 236.05669003725052\n",
      "F1 Score: 0.8054291013319844\n",
      "Epoch 2, Loss: 96.52062425483018\n",
      "F1 Score: 0.8400372597171648\n",
      "Epoch 3, Loss: 75.30679665179923\n",
      "F1 Score: 0.865259051979988\n",
      "Epoch 4, Loss: 61.57716228766367\n",
      "F1 Score: 0.8679789580858646\n",
      "Epoch 5, Loss: 51.3917783908546\n",
      "F1 Score: 0.8738471951941789\n",
      "Epoch 6, Loss: 41.806507223052904\n",
      "F1 Score: 0.8754761703208329\n",
      "Epoch 7, Loss: 34.225525598507375\n",
      "F1 Score: 0.8731488533468732\n",
      "Epoch 8, Loss: 27.148212340660393\n",
      "F1 Score: 0.8810751415772125\n",
      "Epoch 9, Loss: 21.970354374614544\n",
      "F1 Score: 0.8708673340624211\n",
      "Epoch 10, Loss: 16.900123189319856\n",
      "F1 Score: 0.8765161725067386\n",
      "Epoch 11, Loss: 13.023427899111994\n",
      "F1 Score: 0.8718506738093246\n",
      "Epoch 12, Loss: 10.144888904731488\n",
      "F1 Score: 0.8789456010745468\n",
      "Epoch 13, Loss: 7.762162505532615\n",
      "F1 Score: 0.8778349211702219\n",
      "F1 Score on Test Set: 0.8108963093145871\n",
      "Hidden Dimension:  128\n",
      "Epoch 1, Loss: 201.6799725536257\n",
      "F1 Score: 0.8195672307821841\n",
      "Epoch 2, Loss: 86.4660595851019\n",
      "F1 Score: 0.855689333559437\n",
      "Epoch 3, Loss: 65.60438381833956\n",
      "F1 Score: 0.8711096075778079\n",
      "Epoch 4, Loss: 51.82211573980749\n",
      "F1 Score: 0.8788598574821852\n",
      "Epoch 5, Loss: 40.21664285683073\n",
      "F1 Score: 0.8773171553758005\n",
      "Epoch 6, Loss: 30.681652180850506\n",
      "F1 Score: 0.8755408500890811\n",
      "Epoch 7, Loss: 23.100643111392856\n",
      "F1 Score: 0.8840641166991773\n",
      "Epoch 8, Loss: 15.897032566193957\n",
      "F1 Score: 0.8729811574697174\n",
      "Epoch 9, Loss: 11.345537132961908\n",
      "F1 Score: 0.8769036600757256\n",
      "Epoch 10, Loss: 7.852890144480625\n",
      "F1 Score: 0.8831015592077539\n",
      "Epoch 11, Loss: 5.71182703183149\n",
      "F1 Score: 0.8912381596752368\n",
      "Epoch 12, Loss: 6.298802469071234\n",
      "F1 Score: 0.8855813953488372\n",
      "Epoch 13, Loss: 4.46349468180415\n",
      "F1 Score: 0.8832005424648246\n",
      "Epoch 14, Loss: 4.061304799433856\n",
      "F1 Score: 0.8831037105908208\n",
      "Epoch 15, Loss: 3.4354860303137684\n",
      "F1 Score: 0.883995612924998\n",
      "Epoch 16, Loss: 3.9083215012506116\n",
      "F1 Score: 0.8803771361225693\n",
      "F1 Score on Test Set: 0.8220138462886688\n",
      "Hidden Dimension:  256\n",
      "Epoch 1, Loss: 182.72398010268807\n",
      "F1 Score: 0.8287311536506861\n",
      "Epoch 2, Loss: 81.5215649921447\n",
      "F1 Score: 0.8676333418129398\n",
      "Epoch 3, Loss: 60.41113662812859\n",
      "F1 Score: 0.8734617669523891\n",
      "Epoch 4, Loss: 45.60170870949514\n",
      "F1 Score: 0.8853875476493012\n",
      "Epoch 5, Loss: 33.144615962402895\n",
      "F1 Score: 0.8811981722795734\n",
      "Epoch 6, Loss: 23.698233698960394\n",
      "F1 Score: 0.8854326024137344\n",
      "Epoch 7, Loss: 16.048691132920794\n",
      "F1 Score: 0.8840494408475573\n",
      "Epoch 8, Loss: 10.795082028635079\n",
      "F1 Score: 0.888060333870011\n",
      "Epoch 9, Loss: 8.032094694193802\n",
      "F1 Score: 0.8813019042026675\n",
      "Epoch 10, Loss: 5.9221082359872526\n",
      "F1 Score: 0.8824022814963931\n",
      "Epoch 11, Loss: 4.871576119941892\n",
      "F1 Score: 0.8845250083920778\n",
      "Epoch 12, Loss: 4.393842275618226\n",
      "F1 Score: 0.8858416757802642\n",
      "Epoch 13, Loss: 4.417822186376725\n",
      "F1 Score: 0.8880836002022586\n",
      "Epoch 14, Loss: 3.7419462966936408\n",
      "F1 Score: 0.878609805238415\n",
      "Epoch 15, Loss: 4.393826314739272\n",
      "F1 Score: 0.8867052994037121\n",
      "Epoch 16, Loss: 3.161788995967072\n",
      "F1 Score: 0.8895241298133514\n",
      "Epoch 17, Loss: 3.013251086753371\n",
      "F1 Score: 0.8943061999156474\n",
      "Epoch 18, Loss: 2.968149091622763\n",
      "F1 Score: 0.8920401789482569\n",
      "Epoch 19, Loss: 3.662998314444849\n",
      "F1 Score: 0.8856349406715476\n",
      "Epoch 20, Loss: 2.679464968288812\n",
      "F1 Score: 0.8929265613115862\n",
      "Epoch 21, Loss: 2.506319576194983\n",
      "F1 Score: 0.8908370563938296\n",
      "Epoch 22, Loss: 3.5309446023020428\n",
      "F1 Score: 0.8961619569801771\n",
      "Epoch 23, Loss: 2.558868183481536\n",
      "F1 Score: 0.8895041530329726\n",
      "Epoch 24, Loss: 2.4073234776369645\n",
      "F1 Score: 0.8950789229340762\n",
      "Epoch 25, Loss: 2.0918303346215907\n",
      "F1 Score: 0.8927306459773993\n",
      "Epoch 26, Loss: 2.285181301904231\n",
      "F1 Score: 0.8961444360077616\n",
      "Epoch 27, Loss: 3.6455421938453583\n",
      "F1 Score: 0.8893947057831731\n",
      "F1 Score on Test Set: 0.8240879120879121\n",
      "f1 of testset with different hidden dim:  [0.8039890565704705, 0.8108963093145871, 0.8220138462886688, 0.8240879120879121]\n",
      "f1 of trainset with different hidden dim:  [0.874460340303056, 0.8778349211702219, 0.8803771361225693, 0.8893947057831731]\n"
     ]
    }
   ],
   "source": [
    "# find optimal hidden dimension\n",
    "def find_optimal_hidden_dim(parameters):\n",
    "    f1_score_hidden = []\n",
    "    f1_score_test = []\n",
    "    for hidden_dim in parameters:\n",
    "        model = BiLSTMNERModel(EMBEDDING_DIM, hidden_dim, TAGSET_SIZE)\n",
    "        loss_function = nn.NLLLoss(ignore_index=tag2idx['PAD'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(\"Hidden Dimension: \", hidden_dim)\n",
    "        train_loader = DataLoader(train_dataset, optimal_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        validation_loader = DataLoader(validation_dataset, optimal_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        early_stopper = EarlyStopper()\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for sentences, tags in train_loader:\n",
    "                tag_scores = model(sentences)\n",
    "                loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                # Backpropagation\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "            model.eval()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for sentences, tags in validation_loader:\n",
    "                    tag_scores = model(sentences)\n",
    "                    predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "                    # Convert index to tags and compute F1 score\n",
    "                    for sentence, true_seq, pred_seq in zip(sentences, tags.tolist(), predictions):\n",
    "                        valid_len = (sentence != word2idx['<PAD>']).sum().item()\n",
    "                        true_tags = idx_to_tags(true_seq[:valid_len])\n",
    "                        pred_tags = idx_to_tags(pred_seq[:valid_len])\n",
    "                        y_true.append(true_tags)\n",
    "                        y_pred.append(pred_tags)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            print(\"F1 Score:\", f1)\n",
    "            if early_stopper.early_stop(f1):\n",
    "                f1_score_hidden.append(f1)\n",
    "                break\n",
    "        f1_score_test.append(test(model, optimal_batch_size))\n",
    "    print(\"f1 of testset with different hidden dim: \", f1_score_test)\n",
    "    print(\"f1 of trainset with different hidden dim: \", f1_score_hidden)\n",
    "    return f1_score_test\n",
    "\n",
    "hidden_dims = [32, 64, 128, 256]\n",
    "f1_score_hidden = find_optimal_hidden_dim(hidden_dims)\n",
    "optimal_hidden_dim = hidden_dims[np.argmax(f1_score_hidden)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "282b7c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Layers:  1\n",
      "Epoch 1, Loss: 183.8239844245836\n",
      "F1 Score: 0.823187414500684\n",
      "Epoch 2, Loss: 79.49619261920452\n",
      "F1 Score: 0.8633508742149041\n",
      "Epoch 3, Loss: 58.63065435970202\n",
      "F1 Score: 0.8780981284774912\n",
      "Epoch 4, Loss: 44.700760033680126\n",
      "F1 Score: 0.8831124702684336\n",
      "Epoch 5, Loss: 32.25607054203283\n",
      "F1 Score: 0.884514212982605\n",
      "Epoch 6, Loss: 23.299222364788875\n",
      "F1 Score: 0.8850690933602966\n",
      "Epoch 7, Loss: 15.971395267988555\n",
      "F1 Score: 0.8807416772018541\n",
      "Epoch 8, Loss: 10.239005498384358\n",
      "F1 Score: 0.8859470468431772\n",
      "Epoch 9, Loss: 7.2644547176896594\n",
      "F1 Score: 0.8842443729903536\n",
      "Epoch 10, Loss: 5.662726964641479\n",
      "F1 Score: 0.8845795579642645\n",
      "Epoch 11, Loss: 5.449617199505155\n",
      "F1 Score: 0.8761440927029978\n",
      "Epoch 12, Loss: 4.49406147076661\n",
      "F1 Score: 0.8848823429829017\n",
      "Epoch 13, Loss: 4.005842244201631\n",
      "F1 Score: 0.8899796885578876\n",
      "Epoch 14, Loss: 4.083229969583044\n",
      "F1 Score: 0.8824871166680748\n",
      "Epoch 15, Loss: 3.390537276485702\n",
      "F1 Score: 0.8860865910046238\n",
      "Epoch 16, Loss: 3.7772921284158656\n",
      "F1 Score: 0.8853894736842105\n",
      "Epoch 17, Loss: 3.3552272209344665\n",
      "F1 Score: 0.8874228199272605\n",
      "Epoch 18, Loss: 2.915004520873481\n",
      "F1 Score: 0.8923622845555931\n",
      "Epoch 19, Loss: 2.771932521216513\n",
      "F1 Score: 0.887449528936743\n",
      "Epoch 20, Loss: 2.9095491255211527\n",
      "F1 Score: 0.8910272642863173\n",
      "Epoch 21, Loss: 2.9081112706744534\n",
      "F1 Score: 0.8843594710688117\n",
      "Epoch 22, Loss: 2.500832564795928\n",
      "F1 Score: 0.887569950822452\n",
      "Epoch 23, Loss: 3.105268655668624\n",
      "F1 Score: 0.8802561725794219\n",
      "F1 Score on Test Set: 0.8161040787623066\n",
      "Number of Layers:  2\n",
      "Epoch 1, Loss: 169.55274567008018\n",
      "F1 Score: 0.8444780889898226\n",
      "Epoch 2, Loss: 71.00228606630117\n",
      "F1 Score: 0.8813096862210095\n",
      "Epoch 3, Loss: 50.62768403720111\n",
      "F1 Score: 0.8889839938371994\n",
      "Epoch 4, Loss: 36.87816082499921\n",
      "F1 Score: 0.8896821349651539\n",
      "Epoch 5, Loss: 25.625493250467116\n",
      "F1 Score: 0.8895078922934075\n",
      "Epoch 6, Loss: 18.34704770834651\n",
      "F1 Score: 0.8933693268095158\n",
      "Epoch 7, Loss: 12.351524414043524\n",
      "F1 Score: 0.8968853432911822\n",
      "Epoch 8, Loss: 9.615098610534915\n",
      "F1 Score: 0.898135593220339\n",
      "Epoch 9, Loss: 8.443217716379877\n",
      "F1 Score: 0.8908267650511488\n",
      "Epoch 10, Loss: 6.077927978501975\n",
      "F1 Score: 0.8955951580363147\n",
      "Epoch 11, Loss: 7.010272999141307\n",
      "F1 Score: 0.8994673205377526\n",
      "Epoch 12, Loss: 5.1805263735914195\n",
      "F1 Score: 0.8998560907474815\n",
      "Epoch 13, Loss: 3.3974659126251936\n",
      "F1 Score: 0.9031658927817645\n",
      "Epoch 14, Loss: 3.9990769636096957\n",
      "F1 Score: 0.8906053228108471\n",
      "Epoch 15, Loss: 5.046562064778755\n",
      "F1 Score: 0.8996627318718381\n",
      "Epoch 16, Loss: 4.961679319952054\n",
      "F1 Score: 0.8933933933933935\n",
      "Epoch 17, Loss: 3.028747362524882\n",
      "F1 Score: 0.8928032209360844\n",
      "Epoch 18, Loss: 5.271079157847453\n",
      "F1 Score: 0.8963373655913978\n",
      "F1 Score on Test Set: 0.8490698490698491\n",
      "Number of Layers:  3\n",
      "Epoch 1, Loss: 176.23525789100677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephanie/GitHub/SC4002_G06/venv/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8492056749638943\n",
      "Epoch 2, Loss: 71.19694637879729\n",
      "F1 Score: 0.8822287885183623\n",
      "Epoch 3, Loss: 51.88021343154833\n",
      "F1 Score: 0.8898604338321844\n",
      "Epoch 4, Loss: 38.12816311826464\n",
      "F1 Score: 0.8943459915611813\n",
      "Epoch 5, Loss: 28.71021058491897\n",
      "F1 Score: 0.8878931349340548\n",
      "Epoch 6, Loss: 22.24855596909765\n",
      "F1 Score: 0.8938105263157894\n",
      "Epoch 7, Loss: 17.218069708556868\n",
      "F1 Score: 0.896849593495935\n",
      "Epoch 8, Loss: 13.590502724488033\n",
      "F1 Score: 0.8964997487857981\n",
      "Epoch 9, Loss: 11.288492613952258\n",
      "F1 Score: 0.8931116389548693\n",
      "Epoch 10, Loss: 9.044924925932719\n",
      "F1 Score: 0.8977702986958349\n",
      "Epoch 11, Loss: 7.881069247639971\n",
      "F1 Score: 0.8990872210953347\n",
      "Epoch 12, Loss: 9.99096315215138\n",
      "F1 Score: 0.9045099521289998\n",
      "Epoch 13, Loss: 3.6907899444704526\n",
      "F1 Score: 0.9063997981666808\n",
      "Epoch 14, Loss: 2.7473539108609657\n",
      "F1 Score: 0.899613250378342\n",
      "Epoch 15, Loss: 2.9962779560319177\n",
      "F1 Score: 0.9026743398781314\n",
      "Epoch 16, Loss: 3.7344546610993348\n",
      "F1 Score: 0.8995713925539962\n",
      "Epoch 17, Loss: 4.789533712053526\n",
      "F1 Score: 0.9048543689320389\n",
      "Epoch 18, Loss: 3.8788887606142453\n",
      "F1 Score: 0.8992926911417987\n",
      "F1 Score on Test Set: 0.8512666607820637\n",
      "f1 of testset with different hidden dim:  [0.8161040787623066, 0.8490698490698491, 0.8512666607820637]\n",
      "f1 of trainset with different hidden dim:  [0.8802561725794219, 0.8963373655913978, 0.8992926911417987]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find optimal number of layers\n",
    "def find_optimal_num_of_layers(parameters):\n",
    "    f1_score_layers = []\n",
    "    f1_score_test = []\n",
    "    for num_of_layers in parameters:\n",
    "        model = BiLSTMNERModel(EMBEDDING_DIM, optimal_hidden_dim, num_of_layers, TAGSET_SIZE)\n",
    "        loss_function = nn.NLLLoss(ignore_index=tag2idx['PAD'])\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(\"Number of Layers: \", num_of_layers)\n",
    "        train_loader = DataLoader(train_dataset, optimal_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        validation_loader = DataLoader(validation_dataset, optimal_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        early_stopper = EarlyStopper()\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for sentences, tags in train_loader:\n",
    "                tag_scores = model(sentences)\n",
    "                loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                # Backpropagation\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "            model.eval()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for sentences, tags in validation_loader:\n",
    "                    tag_scores = model(sentences)\n",
    "                    predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "                    # Convert index to tags and compute F1 score\n",
    "                    for sentence, true_seq, pred_seq in zip(sentences, tags.tolist(), predictions):\n",
    "                        valid_len = (sentence != word2idx['<PAD>']).sum().item()\n",
    "                        true_tags = idx_to_tags(true_seq[:valid_len])\n",
    "                        pred_tags = idx_to_tags(pred_seq[:valid_len])\n",
    "                        y_true.append(true_tags)\n",
    "                        y_pred.append(pred_tags)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            print(\"F1 Score:\", f1)\n",
    "            if early_stopper.early_stop(f1):\n",
    "                f1_score_layers.append(f1)\n",
    "                break\n",
    "        f1_score_test.append(test(model, optimal_batch_size))\n",
    "    print(\"f1 of testset with different number of layers: \", f1_score_test)\n",
    "    print(\"f1 of trainset with different number of layers: \", f1_score_layers)\n",
    "    return f1_score_test\n",
    "\n",
    "num_of_layers = [1,2,3,4,5]\n",
    "f1_score_layers = find_optimal_num_of_layers(num_of_layers)\n",
    "optimal_num_of_layers = num_of_layers[np.argmax(f1_score_layers)]\n",
    "optimal_num_of_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b4dc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training\n",
      "Epoch 1, Loss: 211.2243566084653\n",
      "F1 Score on Test Set: 0.8203576830678887\n",
      "Epoch 2, Loss: 85.49815228581429\n",
      "F1 Score on Test Set: 0.8310655592860929\n",
      "Epoch 3, Loss: 63.95651657227427\n",
      "F1 Score on Test Set: 0.8478935698447895\n",
      "Epoch 4, Loss: 49.66351771284826\n",
      "F1 Score on Test Set: 0.8482000354672814\n",
      "Epoch 5, Loss: 38.52881736995187\n",
      "F1 Score on Test Set: 0.8559714795008913\n",
      "Epoch 6, Loss: 31.395666539494414\n",
      "F1 Score on Test Set: 0.8584579976985041\n",
      "Epoch 7, Loss: 24.271331545722205\n",
      "F1 Score on Test Set: 0.8539925306775743\n",
      "Epoch 8, Loss: 19.308925223827828\n",
      "F1 Score on Test Set: 0.8639305026150165\n",
      "Epoch 9, Loss: 17.019748357968638\n",
      "F1 Score on Test Set: 0.8573200992555832\n",
      "Epoch 10, Loss: 13.347994626405125\n",
      "F1 Score on Test Set: 0.8584507042253522\n",
      "Epoch 11, Loss: 12.555585913782124\n",
      "F1 Score on Test Set: 0.8533427912750488\n",
      "Epoch 12, Loss: 11.178530612298346\n",
      "F1 Score on Test Set: 0.8586966211999643\n",
      "Epoch 13, Loss: 9.511184494767804\n",
      "F1 Score on Test Set: 0.8604405286343613\n",
      "F1 Score on Test Set: 0.8604405286343613\n"
     ]
    }
   ],
   "source": [
    "# Final training\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "model = BiLSTMNERModel(EMBEDDING_DIM, optimal_hidden_dim, optimal_num_of_layers, TAGSET_SIZE)\n",
    "loss_function = nn.NLLLoss(ignore_index=tag2idx['PAD'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Final Training\")\n",
    "\n",
    "merged_train_dataset = ConcatDataset([train_dataset, validation_dataset])\n",
    "merged_train_loader = DataLoader(merged_train_dataset, optimal_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for sentences, tags in merged_train_loader:\n",
    "        tag_scores = model(sentences)\n",
    "        loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "    f1_test = test(model, optimal_batch_size)\n",
    "    if early_stopper.early_stop(f1_test):\n",
    "        break\n",
    "print(\"F1 Score on Test Set:\", f1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04df04-e0c1-46dd-86db-6c85d1418ce8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc649952-2d63-40bd-87cf-56b6e68f2268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def infer(sentence):\n",
    "#     # Tokenize the sentence\n",
    "#     tokens = sentence.split()\n",
    "\n",
    "#     # Convert tokens to indices\n",
    "#     token_indices = torch.tensor([sentence_to_indices(tokens, word2idx)])\n",
    "\n",
    "#     # Get predictions from the model\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         tag_scores = model(token_indices)\n",
    "#         predictions = tag_scores.argmax(dim=-1).tolist()[0]\n",
    "\n",
    "#     # Convert index to tags\n",
    "#     predicted_tags = idx_to_tags(predictions, {v: k for k, v in tag2idx.items()})\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Display the results\n",
    "#     for token, tag in zip(tokens, predicted_tags):\n",
    "#         print(f\"{token}: {tag}\")\n",
    "#     \"\"\"\n",
    "#     # Prepare aligned output\n",
    "#     token_line = \"\"\n",
    "#     tag_line = \"\"\n",
    "#     for token, tag in zip(tokens, predicted_tags):\n",
    "#         space_padding = max(len(token), len(tag)) + 2  # +2 to add some space between words for better readability\n",
    "#         token_line += token.ljust(space_padding)\n",
    "#         tag_line += tag.ljust(space_padding)\n",
    "\n",
    "#     # Display the results\n",
    "#     print(token_line)\n",
    "#     print(tag_line)\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# sentence = \"EU rejects German call to boycott British lamb .\"\n",
    "# sentence = \"Barack Obama was born in Hawaii and worked as the President of the United States.\"\n",
    "# infer(sentence)\n",
    "\n",
    "# sentence = \"Jiang Yuxin was born in Shenyang and is now a student in Nanyang Technological University.\"\n",
    "# infer(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045ea8-7f99-40c0-9d46-48433cb9dc61",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "e.g. f1 score per class: https://medium.com/illuin/named-entity-recognition-with-bilstm-cnns-632ba83d3d41\n",
    "## data report\n",
    "https://github.com/senadkurtisi/pytorch-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093241f-ed65-4106-8920-81120c4c9626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
