{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f62925-5a23-4b8f-98d7-cc06b1bd28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up prompts\n",
    "SYSTEM_PROMPT = \"You are a smart and intelligent Named Entity Recognition (NER) system using IOB1 tagging scheme. I will provide you the definition of the entities you need to extract, the sentence from where your extract the entities and the output format with examples.\"\n",
    "\n",
    "USER_PROMPT_1 = \"Are you clear about your role?\"\n",
    "\n",
    "ASSISTANT_PROMPT_1 = \"Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started.\"\n",
    "\n",
    "GUIDELINES_PROMPT = (\n",
    "    \"Entity Definition:\\n\"\n",
    "    \"1. B-LOC/I-LOC: These tags are used for any geographic location, like cities, countries, continents, districts etc.\\n\"\n",
    "    \"2. B-ORG/I-ORG: These tags are used for organizations.\\n\"\n",
    "    \"3. B-PER/I-PER: These tags are for names of people.\\n\"\n",
    "    \"3. B-MISC/I-MISC: These tags are for words that are part of entities that do not fall under the other predefined categories (Location, Organization, Person).\\n\"\n",
    "    \"4. O: This tag is for words that do not belong to any of the entity categories mentioned above.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Annotation Rules:\\n\"\n",
    "    \"Single-Word Entities: If an entity consists of only one word, it should be tagged with an 'I-' prefix.\\n\"\n",
    "    \"Multi-Word Entities: If an entity contains multiple words, the first word should be tagged with 'I-' and the subsequent words with 'I-' as well.\\n\"\n",
    "    \"Separate Entities: If two entities of the same type are adjacent to each other, the first word of the second entity should be tagged with 'B-' to indicate a new entity.\\n\"\n",
    "    \"Continuous Entities: Do not use a 'B-' tag. Instead, start multi-word entities with 'I-' and continue with 'I-' tags for all subsequent words that belong to the entity.\\n\"\n",
    "    \"'B-' prefix is only used to separate two adjacent entities of the same type.\\n\"\n",
    "    \"Entity Ambiguity: If a word could belong to more than one category, prioritize by context. If still unsure, maintain consistency with the rest of the document or seek clarification.\\n\"\n",
    "    \"Punctuation: Punctuation is typically tagged as 'O' unless part of a named entity.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Output Format:\\n\"\n",
    "    \"[list of name entities corresponding to each word in the sentence]\"\n",
    "    \"\\n\"\n",
    "    \"Output Format and Examples:\\n\"\n",
    "    \"\\n\"\n",
    "    \"1. Sentence: ['Swiss', 'Grand', 'Prix', 'World', 'Cup', 'cycling', 'race', 'on', 'Sunday', ':']\\n\"\n",
    "    \"Output: ['I-MISC', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O']\\n\"\n",
    "    \"\\n\"\n",
    "    \"2. Sentence: ['After', 'the', 'defeat', 'of', 'the', 'resolution', ',', 'drafted', 'by', 'the', 'European', 'Union', 'and', 'the', 'United', 'States', ',', 'China', \\''s\\', 'Foreign', 'Ministry', 'thanked', '26', 'countries', 'for', 'backing', 'its', 'motion', 'for', '\\\"', 'no', 'action', '\\\"', 'on', 'the', 'document', '.']\"\n",
    "    \"Output: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'I-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\\n\"\n",
    "    \"\\n\"\n",
    "    \"I will provide you a list of lists of tokens. Please return a list of lists of labeled BIO tags, which should have the same length as input tokens, without unnecessary explanations.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dcc63d-16f4-48ae-92cc-45b540a34493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 setup\n",
    "import openai\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "models = openai.Model.list()\n",
    "# for model in models['data']:\n",
    "#    print(model['id'])  # This prints the ID of each model\n",
    "\n",
    "def openai_chat_completion_response(final_prompt):\n",
    "  response = openai.ChatCompletion.create(\n",
    "              model=\"gpt-4\",\n",
    "              messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": USER_PROMPT_1},\n",
    "                    {\"role\": \"assistant\", \"content\": ASSISTANT_PROMPT_1},\n",
    "                    {\"role\": \"user\", \"content\": final_prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "  return response['choices'][0]['message']['content'].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5980a5c-90cc-4f27-b623-46957abe23f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# read data\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data\n",
    "\n",
    "# prepare data\n",
    "def convert_to_dataset(data):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": [], \"sentences\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [token_data[3] for token_data in sentence]\n",
    "        sentence_str = \" \".join(tokens)\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "        formatted_data[\"sentences\"].append(sentence_str)\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "test_data = read_conll_file(\"/mnt/lustre/yuxin/SC4002_G06/datasets/CoNLL2003/eng.testb\")\n",
    "test_dataset = convert_to_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9007f54-640d-47a6-ae3f-a7a2fd89e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = GUIDELINES_PROMPT + str(test_dataset[\"tokens\"][0:5])\n",
    "replies = openai_chat_completion_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a0f48c-c8e4-4e4f-b769-8ed1d213c971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O'], ['B-PER', 'I-PER'], ['B-LOC', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O'], ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83db5405-de03-487c-a966-25d99c75d7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761904761904762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.86      0.80         7\n",
      "        MISC       0.50      1.00      0.67         1\n",
      "         PER       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.73      0.80      0.76        10\n",
      "   macro avg       0.75      0.79      0.71        10\n",
      "weighted avg       0.78      0.80      0.76        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "\n",
    "# Function to pad or clip a list to match the length of another list\n",
    "def pad_or_clip_to_match_length(source_list, target_length):\n",
    "    if len(source_list) < target_length:\n",
    "        return source_list + ['O'] * (target_length - len(source_list))\n",
    "    elif len(source_list) > target_length:\n",
    "        return source_list[:target_length]\n",
    "    else:\n",
    "        return source_list\n",
    "\n",
    "y_true = test_dataset[\"ner_tags\"][0:5]\n",
    "reply_list = list(ast.literal_eval(replies))\n",
    "\n",
    "# Ensure that list2 has sublists with the same length as list1\n",
    "y_pred = [pad_or_clip_to_match_length(sublist2, len(sublist1)) for sublist1, sublist2 in zip(y_true, reply_list)]\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f1)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00698eff-87fc-46e0-9fe6-f17ea924d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = GUIDELINES_PROMPT + str(test_dataset[\"tokens\"][100:110])\n",
    "replies = openai_chat_completion_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6af9e17-d626-4145-a39c-2fc6203b56f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[['O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'],\\n['O', 'O', 'O'],\\n['I-LOC'],\\n['I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\\n['I-PER', 'I-PER', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O'],\\n['I-PER', 'I-PER', 'O', 'I-PER', 'O', 'I-PER', 'O'],\\n['I-PER', 'I-PER', 'O', 'I-PER', 'O', 'I-PER', 'O'],\\n['I-PER', 'I-PER', 'O', 'I-PER', 'O'],\\n['I-PER', 'I-PER', 'O', 'I-PER', 'O'],\\n['I-PER', 'I-PER', 'O', 'I-PER', 'O', 'I-PER', 'O']]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299f362b-525a-44ae-9549-1bd9ac3f0d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       1.00      1.00      1.00         3\n",
      "         PER       1.00      1.00      1.00        16\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "\n",
    "# Function to pad or clip a list to match the length of another list\n",
    "def pad_or_clip_to_match_length(source_list, target_length):\n",
    "    if len(source_list) < target_length:\n",
    "        return source_list + ['O'] * (target_length - len(source_list))\n",
    "    elif len(source_list) > target_length:\n",
    "        return source_list[:target_length]\n",
    "    else:\n",
    "        return source_list\n",
    "\n",
    "y_true = test_dataset[\"ner_tags\"][100:110]\n",
    "new_string = replies.replace('\\n', '')\n",
    "reply_list = list(ast.literal_eval(new_string))\n",
    "\n",
    "# Ensure that list2 has sublists with the same length as list1\n",
    "y_pred = [pad_or_clip_to_match_length(sublist2, len(sublist1)) for sublist1, sublist2 in zip(y_true, reply_list)]\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f1)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0beaa1b6-4440-44f1-a647-602b8135239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = GUIDELINES_PROMPT + str(test_dataset[\"tokens\"][200:220])\n",
    "replies = openai_chat_completion_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b0bad0-dc4e-4a0c-8718-700479aa2d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[['B-PER', 'I-PER'], \\n['B-LOC', 'O'], \\n['O', 'I-PER', 'I-PER', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'I-LOC', 'O', 'O', 'O'],\\n['I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'I-PER', 'I-PER', 'O'],\\n['I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O'],\\n['O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O'],\\n['O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\\n['I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'I-LOC', 'O', 'I-LOC', 'O'],\\n['O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O'],\\n['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\\n['O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O', 'I-PER', 'I-PER', 'O', 'I-ORG', 'I-ORG', 'I-PER', 'O'],\\n['O', 'O'],\\n['I-ORG', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O'],\\n['I-ORG', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O'],\\n['O'],\\n['I-MISC', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'],\\n['B-LOC', 'O'],\\n['O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'I-LOC', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O'],\\n['B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-PER'],\\n['I-PER', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O']]\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a61f9a0-2995-483d-badd-21ab7c525943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2909090909090909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.23      0.25      0.24        36\n",
      "        MISC       0.00      0.00      0.00         7\n",
      "         ORG       0.45      0.71      0.56         7\n",
      "         PER       0.32      0.30      0.31        60\n",
      "\n",
      "   micro avg       0.29      0.29      0.29       110\n",
      "   macro avg       0.25      0.32      0.28       110\n",
      "weighted avg       0.28      0.29      0.28       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "\n",
    "# Function to pad or clip a list to match the length of another list\n",
    "def pad_or_clip_to_match_length(source_list, target_length):\n",
    "    if len(source_list) < target_length:\n",
    "        return source_list + ['O'] * (target_length - len(source_list))\n",
    "    elif len(source_list) > target_length:\n",
    "        return source_list[:target_length]\n",
    "    else:\n",
    "        return source_list\n",
    "\n",
    "y_true = test_dataset[\"ner_tags\"][200:220]\n",
    "new_string = replies.replace('\\n', '')\n",
    "reply_list = list(ast.literal_eval(new_string))\n",
    "\n",
    "# Ensure that list2 has sublists with the same length as list1\n",
    "y_pred = [pad_or_clip_to_match_length(sublist2, len(sublist1)) for sublist1, sublist2 in zip(y_true, reply_list)]\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f1)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd6cbf-0e60-4c61-8409-591f49786671",
   "metadata": {},
   "source": [
    "# Let's Go GPT-4!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd813372-2926-4499-a9f0-7204190cb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                      | 1/369 [00:38<3:56:29, 38.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 1: 0.46511627906976744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                                      | 2/369 [01:23<4:18:26, 42.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 2: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                      | 3/369 [02:04<4:13:39, 41.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 3: 0.6530612244897959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                      | 4/369 [02:55<4:36:06, 45.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 4: 0.619718309859155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                      | 5/369 [04:04<5:27:39, 54.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 5: 0.5714285714285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                                      | 6/369 [04:19<4:05:18, 40.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 6: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                                      | 7/369 [04:36<3:17:59, 32.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 7: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                     | 8/369 [04:54<2:50:49, 28.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 8: 0.9142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                     | 9/369 [05:12<2:29:29, 24.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for batch 9: 0.9600000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                     | 9/369 [05:29<3:39:45, 36.63s/it]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[37], line 31\u001b[0m\n    y_pred_batch = clean_predictions(y_true_batch, predictions)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[37], line 17\u001b[0m in \u001b[1;35mclean_predictions\u001b[0m\n    reply_list = list(ast.literal_eval(new_string))\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/ast.py:59\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/ast.py:47\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Here are the tagged entities for your provided sentences:[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O'], ['B-MISC', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC', 'O', 'O', 'O'], ['O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['I-LOC', 'O', 'I-LOC', 'O'], ['O', 'O', 'O', 'O']]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to pad or clip a list to match the length of another list\n",
    "def pad_or_clip_to_match_length(source_list, target_length):\n",
    "    if len(source_list) < target_length:\n",
    "        return source_list + ['O'] * (target_length - len(source_list))\n",
    "    elif len(source_list) > target_length:\n",
    "        return source_list[:target_length]\n",
    "    else:\n",
    "        return source_list\n",
    "\n",
    "def clean_predictions(y_true, replies):\n",
    "    new_string = replies.replace('\\n', '')\n",
    "    reply_list = list(ast.literal_eval(new_string))\n",
    "    \n",
    "    # Ensure that list2 has sublists with the same length as list1\n",
    "    y_pred = [pad_or_clip_to_match_length(sublist2, len(sublist1)) for sublist1, sublist2 in zip(y_true, reply_list)]\n",
    "    return y_pred\n",
    "\n",
    "y_trues = []\n",
    "y_preds = []\n",
    "for i in tqdm(range(0, len(test_dataset), 10)):\n",
    "    # Create the prompt using a subset of the test dataset\n",
    "    prompt = GUIDELINES_PROMPT + str(test_dataset[\"tokens\"][i:i+10])\n",
    "    predictions = openai_chat_completion_response(prompt)\n",
    "    \n",
    "    y_true_batch = test_dataset[\"ner_tags\"][i:i+10]\n",
    "    y_pred_batch = clean_predictions(y_true_batch, predictions)\n",
    "\n",
    "    batch_f1 = f1_score(y_true_batch, y_pred_batch)\n",
    "    print(f\"F1 score for batch {i // 10 + 1}: {batch_f1}\")\n",
    "    \n",
    "    # Extend our lists of true and predicted labels\n",
    "    y_trues.extend(y_true_batch)\n",
    "    y_preds.extend(y_pred_batch)\n",
    "\n",
    "# Once the loop is complete, we can calculate the F1\n",
    "f1 = f1_score(y_trues, y_preds)\n",
    "print(f\"The overall F1 score is: {f1}\")\n",
    "print(classification_report(y_trues, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24256c56-1a4d-43c2-853d-e337e94f9ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002",
   "language": "python",
   "name": "sc4002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
