{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f23042a-5f2e-483f-b888-8ff2fcb9cfe3",
   "metadata": {},
   "source": [
    "# 1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6eb528a-a228-4d55-b62d-bd22b3a9c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cd298-8b17-4757-ac41-fdf6d4fb8915",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "use cosine similarity to find the most similar \n",
    "word to each of these worsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab65046-7971-4d01-abd7-28135a3bb217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\tMost similar word\tCosine similarity\n",
      "=======================================================================\n",
      "student        \tstudents       \t\t0.7295\n",
      "Apple          \tApple_AAPL     \t\t0.7457\n",
      "apple          \tapples         \t\t0.7204\n"
     ]
    }
   ],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "\n",
    "# Print the header\n",
    "print(\"Word\\t\\tMost similar word\\tCosine similarity\")\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "for word in words:\n",
    "    # Use the downloaded vectors as usual:\n",
    "    most_similar = w2v.most_similar(positive=[word], topn=1)[0]\n",
    "    print(\"{:<15}\\t{:<15}\\t\\t{:.4f}\".format(word, most_similar[0], most_similar[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed0dd1-ea8d-4de5-bffc-86bc0ecc51ee",
   "metadata": {},
   "source": [
    "# 1.2 Data\n",
    "process: https://wandb.ai/mostafaibrahim17/ml-articles/reports/Named-Entity-Recognition-With-HuggingFace-Using-PyTorch-and-W-B--Vmlldzo0NDgzODA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79fcdfa-f1da-4c96-a145-298ab4952340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_conll_file(\"/content/drive/MyDrive/CoNLL2003/eng.train\")\n",
    "validation_data = read_conll_file(\"/content/drive/MyDrive/CoNLL2003/eng.testa\")\n",
    "test_data = read_conll_file(\"/content/drive/MyDrive/CoNLL2003/eng.testb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5137c1c0-eecc-4ee5-a7b7-a0101c7a4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def convert_to_dataset(data, label_map):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "\n",
    "label_list = sorted(list(set([token_data[3] for sentence in train_data for token_data in sentence])))\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "train_dataset = convert_to_dataset(train_data, label_map)\n",
    "validation_dataset = convert_to_dataset(validation_data, label_map)\n",
    "test_dataset = convert_to_dataset(test_data, label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d226892-7cd6-44c6-abf1-d770852968cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-MISC': 1,\n",
       " 'B-ORG': 2,\n",
       " 'I-LOC': 3,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 5,\n",
       " 'I-PER': 6,\n",
       " 'O': 7}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7418a-e9e0-4f0c-91c8-b8d5193aedb3",
   "metadata": {},
   "source": [
    "## Question 1.2\n",
    "(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
    "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
    "etc.) you chos\n",
    "\n",
    "(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
    "entities with more than one word. Explain how to form complete named entities from the label\n",
    "for each word, and list all the named entities in this sentence.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed55a4ea-ffcb-49c5-bc67-1f90ab083da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sizes:\n",
      "Training:\t14987 sentences\n",
      "Development:\t3466 sentences\n",
      "Test:\t\t3684 sentences\n",
      "=======================================================================\n",
      "All Possible Word Labels (BIO):\n",
      " ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "print(\"Dataset Sizes:\")\n",
    "print(f\"Training:\\t{train_dataset.num_rows} sentences\")\n",
    "print(f\"Development:\\t{validation_dataset.num_rows} sentences\")\n",
    "print(f\"Test:\\t\\t{test_dataset.num_rows} sentences\")\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "print(\"All Possible Word Labels (BIO):\\n\", label_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf2505-7f3a-4fd2-a736-a360b94dae30",
   "metadata": {},
   "source": [
    "!! (b) means finding the sentence that contains at least two distinct named entities, and each of those entities consists of more than one word.\n",
    "=> but seems in training dataset, there isn't this kind of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f8cd84b-8caf-4486-ab9e-e000ef503aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████▉                              | 5969/14987 [19:27<29:24,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5969\n",
      "['Swiss', 'Grand', 'Prix', 'World', 'Cup', 'cycling', 'race', 'on', 'Sunday', ':']\n",
      "[4, 1, 4, 1, 4, 7, 7, 7, 7, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "lists2 = [0,1,2] # ['B-LOC', 'B-MISC', 'B-ORG']\n",
    "\n",
    "def has_at_least_two_common_elements(list1, list2=[0,1,2]):\n",
    "    common_elements = [value for value in list1 if value in list2]\n",
    "    return len(common_elements) >= 2\n",
    "\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    tokens = train_dataset['tokens'][i]\n",
    "    ner_tags = train_dataset['ner_tags'][i]\n",
    "\n",
    "    if has_at_least_two_common_elements(ner_tags):\n",
    "        print(i)\n",
    "        print(tokens)\n",
    "        print(ner_tags)\n",
    "        print([label_list[tag] for tag in ner_tags])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f82114d-a819-4908-8143-0087bcb9938b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-MISC', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[label_list[tag] for tag in train_dataset[5969]['ner_tags']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fe1611-8fe9-45aa-a8df-ad2e8d8a7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Swiss', 'Grand Prix', 'World Cup'],\n",
       " ['I-MISC', 'B-MISC I-MISC', 'B-MISC I-MISC'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (b)\n",
    "def form_complete_ne(dataset, i):\n",
    "    # define sets of tags\n",
    "    begin_tags = {'B-LOC', 'B-ORG', 'B-MISC'}\n",
    "    inside_tags = {'I-ORG', 'I-LOC', 'I-PER', 'I-MISC'}\n",
    "    outside_tags = {'O'}\n",
    "\n",
    "    words = []\n",
    "    word = []\n",
    "    entities = []\n",
    "    entity = []\n",
    "\n",
    "    tokens = dataset['tokens'][i]\n",
    "    ner_tags = dataset['ner_tags'][i]\n",
    "\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        tag = label_list[tag]\n",
    "\n",
    "        if (tag in begin_tags or tag in outside_tags) and word:\n",
    "            words.append(' '.join(word))\n",
    "            entities.append(' '.join(entity))\n",
    "            word = []\n",
    "            entity = []\n",
    "\n",
    "        if tag in begin_tags or tag in inside_tags:\n",
    "            word.append(token)\n",
    "            entity.append(tag)\n",
    "\n",
    "    if word:\n",
    "        words.append(' '.join(word))\n",
    "        entities.append(' '.join(entity))\n",
    "\n",
    "    return words, entities\n",
    "\n",
    "form_complete_ne(train_dataset, 5969)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7891e6-272f-49fa-9d63-19049f999a10",
   "metadata": {},
   "source": [
    "# 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db447bac-3109-4c13-8257-3bca386dde26",
   "metadata": {},
   "source": [
    "1. `<PAD>` Token:\n",
    "\n",
    "This token is typically initialized to a zero vector because it's meant to be a neutral padding value that doesn't interfere with computation\n",
    "\n",
    "2. `<UNK>` Token:\n",
    "- Zero Vector: Similar to the <PAD> token, you can initialize it to a zero vector.\n",
    "- Average Vector: Initialize it as the average of all word vectors in your pretrained embeddings. This gives it a kind of \"average\" representation of the language.\n",
    "- Random Vector: Randomly initialize it, which might add some noise and robustness to the embeddi\n",
    "\n",
    "For many tasks, initializing the <UNK> token as the average of all word vectors works well. It makes the <UNK> token have a representation that is, on average, similar to any random word from the vocabulary, which can be beneficial since the <UNK> token is used for words that aren't in the training vocabulary but could be anywhere in the semantic space.ngs.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "606d6017-bbc5-476a-bc69-1356030f91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c36319-becd-4f14-be61-614cc35852e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after insert UNK:  (3000001, 300)\n",
      "after insert UNK:  (3000002, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Out-of-vocabulary (OOV) words\n",
    "# 1. can be replaced with a special token, such as \"<OOV>\" or \"<UNK>\".\n",
    "# 2. can be ignored.\n",
    "\n",
    "word2idx = w2v.key_to_index\n",
    "print(f\"whether <UNK> in w2v: {'<UNK>' in word2idx}\") # False\n",
    "print(f\"whether <PAD> in w2v: {'<PAD>' in word2idx}\") # False\n",
    "\n",
    "# Add '<UNK>' and '<PAD>' tokens to the vocabulary index\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "print(f\"word2idx['<UNK>']: {word2idx['<UNK>']}\")\n",
    "print(f\"word2idx['<PAD>']: {word2idx['<PAD>']}\")\n",
    "\n",
    "# add the '<UNK>' word to the vocabulary of the Word2Vec model\n",
    "# initialize it with the average of all word vectors in the pretrained embeddings.\n",
    "unk_vector = np.mean(w2v.vectors, axis=0)\n",
    "w2v.vectors = np.vstack([w2v.vectors, unk_vector])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n",
    "\n",
    "# add the '<PAD>' word to the vocabulary of the Word2Vec model\n",
    "# initialize it with a row of zeros in the vectors matrix.\n",
    "w2v.vectors = np.vstack([w2v.vectors, np.zeros(w2v.vectors[0].shape)])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fbbae-6788-4067-85c8-23ed010199e6",
   "metadata": {},
   "source": [
    "# Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3beb52ff-de17-4f67-899c-6b7e8dc58802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to Indices\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.get(word, vocab.get('<UNK>')) for word in sentence]\n",
    "\n",
    "tag2idx = {\n",
    "    'B-LOC': 0,\n",
    "    'B-MISC': 1,\n",
    "    'B-ORG': 2,\n",
    "    'I-LOC': 3,\n",
    "    'I-MISC': 4,\n",
    "    'I-ORG': 5,\n",
    "    'I-PER': 6,\n",
    "    'O': 7,\n",
    "    'PAD': 8\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a55757fa-5020-481d-b1c1-ba3faa21062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, vocab):\n",
    "        self.sentences = [torch.tensor(sentence_to_indices(sentence, vocab)) for sentence in sentences]\n",
    "        self.tags = [torch.tensor(tag) for tag in tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = NERDataset(train_dataset['tokens'], train_dataset['ner_tags'], word2idx)\n",
    "validation_dataset = NERDataset(validation_dataset['tokens'], validation_dataset['ner_tags'], word2idx)\n",
    "test_dataset = NERDataset(test_dataset['tokens'], test_dataset['ner_tags'], word2idx)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=tag2idx['PAD'])\n",
    "    return sentences_padded, tags_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d01a3-0621-413d-b675-ae79a6ff8866",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cee6066-7544-4b7b-97b5-418a7788f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(w2v.vectors)\n",
    "\n",
    "class LSTMNERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMNERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        tag_scores = torch.log_softmax(tag_space, dim=-1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0cb07-a595-46c5-bf81-9331df1ba55c",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a66728d-97bf-4e65-a9e6-304671240573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = w2v[0].shape[0]\n",
    "HIDDEN_DIM = 150\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "MAX_EPOCHS = 200\n",
    "\n",
    "model = LSTMNERModel(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def early_stop(self, f1):\n",
    "        if f1 > self.max_f1:\n",
    "            self.max_f1 = f1\n",
    "            self.counter = 0\n",
    "        elif f1 < (self.max_f1):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d1f82-b9ee-4532-8a5b-37b917992ec7",
   "metadata": {},
   "source": [
    "# Train\n",
    "https://necromuralist.github.io/Neurotic-Networking/posts/nlp/ner-evaluating-the-model/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611036c7-a564-4fe9-8ed0-c34a2c058e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "\n",
    "def idx_to_tags(indices):\n",
    "    idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "    return [idx2tag[idx] for idx in indices]\n",
    "\n",
    "def find_optimal_batch_size(parameters):\n",
    "    f1_score_dev = []\n",
    "    for batch_size in parameters:\n",
    "        model = LSTMNERModel(EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(\"Batch Size: \", batch_size)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        early_stopper = EarlyStopper()\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for sentences, tags in train_loader:\n",
    "                tag_scores = model(sentences)\n",
    "                loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "            model.eval()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for sentences, tags in validation_loader:\n",
    "                    tag_scores = model(sentences)\n",
    "                    predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "                    # Convert index to tags\n",
    "                    tag_seqs = [idx_to_tags(seq) for seq in tags.tolist()]\n",
    "                    pred_seqs = [idx_to_tags(seq) for seq in predictions]\n",
    "                    y_true.extend(tag_seqs)\n",
    "                    y_pred.extend(pred_seqs)\n",
    "            # Compute F1 score\n",
    "            # Remove <PAD> label from true and predicted labels\n",
    "            # filtered_y_true = []\n",
    "            # filtered_y_pred = []\n",
    "            # for true_seq, pred_seq in zip(y_true, y_pred):\n",
    "            #     filtered_true_seq = [label for label in true_seq if label != 'PAD']\n",
    "            #     filtered_pred_seq = [label for label in pred_seq if label != 'PAD']\n",
    "            #     shortest_len = min(len(filtered_true_seq), len(filtered_pred_seq))\n",
    "            #     short_true_seq = filtered_true_seq[:shortest_len]\n",
    "            #     short_pred_seq = filtered_pred_seq[:shortest_len]\n",
    "            #     filtered_y_true.append(short_true_seq)\n",
    "            #     filtered_y_pred.append(short_pred_seq)\n",
    "            #f1 = f1_score(filtered_y_true, filtered_y_pred, mode='strict', scheme=IOB1)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            print(\"F1 Score:\", f1)\n",
    "            if early_stopper.early_stop(f1):\n",
    "                f1_score_dev.append(f1)\n",
    "                break\n",
    "    return f1_score_dev\n",
    "\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "f1_score_dev = find_optimal_batch_size(batch_sizes)\n",
    "optimal_batch_size = batch_sizes[np.argmax(f1_score_dev)]\n",
    "f1_score_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal hidden dimension\n",
    "def find_optimal_hidden_dim(parameters):\n",
    "    f1_score_hidden = []\n",
    "    for hidden_dim in parameters:\n",
    "        model = LSTMNERModel(EMBEDDING_DIM, hidden_dim, TAGSET_SIZE)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        print(\"Hidden Dimension: \", hidden_dim)\n",
    "        train_loader = DataLoader(train_dataset, optimal_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        validation_loader = DataLoader(validation_dataset, optimal_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        early_stopper = EarlyStopper()\n",
    "\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            total_loss = 0\n",
    "            model.train()\n",
    "            for sentences, tags in train_loader:\n",
    "                tag_scores = model(sentences)\n",
    "                loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "                total_loss += loss.item()\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "            model.eval()\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for sentences, tags in validation_loader:\n",
    "                    tag_scores = model(sentences)\n",
    "                    predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "                    # Convert index to tags\n",
    "                    tag_seqs = [idx_to_tags(seq) for seq in tags.tolist()]\n",
    "                    pred_seqs = [idx_to_tags(seq) for seq in predictions]\n",
    "                    y_true.extend(tag_seqs)\n",
    "                    y_pred.extend(pred_seqs)\n",
    "            # Compute F1 score\n",
    "            # Remove <PAD> label from true and predicted labels\n",
    "            # filtered_y_true = []\n",
    "            # filtered_y_pred = []\n",
    "            # for true_seq, pred_seq in zip(y_true, y_pred):\n",
    "            #     filtered_true_seq = [label for label in true_seq if label != 'PAD']\n",
    "            #     filtered_pred_seq = [label for label in pred_seq if label != 'PAD']\n",
    "            #     shortest_len = min(len(filtered_true_seq), len(filtered_pred_seq))\n",
    "            #     short_true_seq = filtered_true_seq[:shortest_len]\n",
    "            #     short_pred_seq = filtered_pred_seq[:shortest_len]\n",
    "            #     filtered_y_true.append(short_true_seq)\n",
    "            #     filtered_y_pred.append(short_pred_seq)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            print(\"F1 Score:\", f1)\n",
    "            if early_stopper.early_stop(f1):\n",
    "                f1_score_hidden.append(f1)\n",
    "                break\n",
    "    return f1_score_hidden\n",
    "\n",
    "hidden_dims = [64, 128, 256, 512]\n",
    "f1_score_hidden = find_optimal_hidden_dim(hidden_dims)\n",
    "optimal_hidden_dim = hidden_dims[np.argmax(f1_score_hidden)]\n",
    "f1_score_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2988d4f-3776-4993-b69b-776b951829bb",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "417a7386-2f72-4037-baca-265d17c621aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Test Set: 0.8473496707330238\n"
     ]
    }
   ],
   "source": [
    "# Placeholder to store true and predicted tags for the test set\n",
    "y_true_test = []\n",
    "y_pred_test = []\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for sentences, tags in test_loader:\n",
    "        tag_scores = model(sentences)\n",
    "        predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "\n",
    "        # Convert index to tags\n",
    "        idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "        tag_seqs = [idx_to_tags(seq, idx2tag) for seq in tags.tolist()]\n",
    "        pred_seqs = [idx_to_tags(seq, idx2tag) for seq in predictions]\n",
    "\n",
    "        y_true_test.extend(tag_seqs)\n",
    "        y_pred_test.extend(pred_seqs)\n",
    "\n",
    "# Compute F1 score for the test set\n",
    "f1_test = f1_score(y_true_test, y_pred_test)\n",
    "# TODO: debugg KeyError: 'P'\n",
    "# report_test = classification_report(y_true_test, y_pred_test, mode='strict', scheme=IOB1)\n",
    "\n",
    "print(\"F1 Score on Test Set:\", f1_test)\n",
    "# print(\"Classification Report on Test Set:\\n\", report_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04df04-e0c1-46dd-86db-6c85d1418ce8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc649952-2d63-40bd-87cf-56b6e68f2268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack  Obama  was  born  in  Hawaii  and  worked  as  the  President  of  the  United  States.  \n",
      "I-PER   I-PER  O    O     O   I-LOC   O    O       O   O    O          O   O    I-LOC   O        \n",
      "Jiang  Yuxin  was  born  in  Shenyang  and  is  now  a  student  in  Nanyang  Technological  University.  \n",
      "I-PER  O      O    O     O   I-LOC     O    O   O    O  O        O   I-LOC    I-ORG          O            \n"
     ]
    }
   ],
   "source": [
    "def infer(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    token_indices = torch.tensor([sentence_to_indices(tokens, word2idx)])\n",
    "\n",
    "    # Get predictions from the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tag_scores = model(token_indices)\n",
    "        predictions = tag_scores.argmax(dim=-1).tolist()[0]\n",
    "\n",
    "    # Convert index to tags\n",
    "    predicted_tags = idx_to_tags(predictions, {v: k for k, v in tag2idx.items()})\n",
    "\n",
    "    \"\"\"\n",
    "    # Display the results\n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        print(f\"{token}: {tag}\")\n",
    "    \"\"\"\n",
    "    # Prepare aligned output\n",
    "    token_line = \"\"\n",
    "    tag_line = \"\"\n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        space_padding = max(len(token), len(tag)) + 2  # +2 to add some space between words for better readability\n",
    "        token_line += token.ljust(space_padding)\n",
    "        tag_line += tag.ljust(space_padding)\n",
    "\n",
    "    # Display the results\n",
    "    print(token_line)\n",
    "    print(tag_line)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"EU rejects German call to boycott British lamb .\"\n",
    "sentence = \"Barack Obama was born in Hawaii and worked as the President of the United States.\"\n",
    "infer(sentence)\n",
    "\n",
    "sentence = \"Jiang Yuxin was born in Shenyang and is now a student in Nanyang Technological University.\"\n",
    "infer(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045ea8-7f99-40c0-9d46-48433cb9dc61",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "e.g. f1 score per class: https://medium.com/illuin/named-entity-recognition-with-bilstm-cnns-632ba83d3d41\n",
    "## data report\n",
    "https://github.com/senadkurtisi/pytorch-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093241f-ed65-4106-8920-81120c4c9626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002",
   "language": "python",
   "name": "sc4002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
