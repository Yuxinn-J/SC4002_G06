{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f23042a-5f2e-483f-b888-8ff2fcb9cfe3",
   "metadata": {},
   "source": [
    "# 1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6eb528a-a228-4d55-b62d-bd22b3a9c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Download the embeddings\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed0dd1-ea8d-4de5-bffc-86bc0ecc51ee",
   "metadata": {},
   "source": [
    "# 1.2 Data\n",
    "process: https://wandb.ai/mostafaibrahim17/ml-articles/reports/Named-Entity-Recognition-With-HuggingFace-Using-PyTorch-and-W-B--Vmlldzo0NDgzODA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79fcdfa-f1da-4c96-a145-298ab4952340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_conll_file(\"/mnt/lustre/yuxin/SC4002_G06/datasets/CoNLL2003/eng.train\")\n",
    "validation_data = read_conll_file(\"/mnt/lustre/yuxin/SC4002_G06/datasets/CoNLL2003/eng.testa\")\n",
    "test_data = read_conll_file(\"/mnt/lustre/yuxin/SC4002_G06/datasets/CoNLL2003/eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5137c1c0-eecc-4ee5-a7b7-a0101c7a4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "def convert_to_dataset(data, label_map):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [label_map[token_data[3]] for token_data in sentence]\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "\n",
    "label_list = sorted(list(set([token_data[3] for sentence in train_data for token_data in sentence])))\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "train_dataset = convert_to_dataset(train_data, label_map)\n",
    "validation_dataset = convert_to_dataset(validation_data, label_map)\n",
    "test_dataset = convert_to_dataset(test_data, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d226892-7cd6-44c6-abf1-d770852968cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-MISC': 1,\n",
       " 'B-ORG': 2,\n",
       " 'I-LOC': 3,\n",
       " 'I-MISC': 4,\n",
       " 'I-ORG': 5,\n",
       " 'I-PER': 6,\n",
       " 'O': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7418a-e9e0-4f0c-91c8-b8d5193aedb3",
   "metadata": {},
   "source": [
    "## Question 1.2\n",
    "(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
    "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
    "etc.) you chos\n",
    "\n",
    "(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
    "entities with more than one word. Explain how to form complete named entities from the label\n",
    "for each word, and list all the named entities in this sentence.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed55a4ea-ffcb-49c5-bc67-1f90ab083da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sizes:\n",
      "Training:\t14987 sentences\n",
      "Development:\t3466 sentences\n",
      "Test:\t\t3684 sentences\n",
      "=======================================================================\n",
      "All Possible Word Labels (BIO):\n",
      " ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "print(\"Dataset Sizes:\")\n",
    "print(f\"Training:\\t{train_dataset.num_rows} sentences\")\n",
    "print(f\"Development:\\t{validation_dataset.num_rows} sentences\")\n",
    "print(f\"Test:\\t\\t{test_dataset.num_rows} sentences\")\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "print(\"All Possible Word Labels (BIO):\\n\", label_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf2505-7f3a-4fd2-a736-a360b94dae30",
   "metadata": {},
   "source": [
    "!! (b) means finding the sentence that contains at least two distinct named entities, and each of those entities consists of more than one word.\n",
    "=> but seems in training dataset, there isn't this kind of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8cd84b-8caf-4486-ab9e-e000ef503aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 14/14987 [00:02<53:22,  4.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(common_elements) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))):\n\u001b[0;32m----> 9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]\n\u001b[1;32m     10\u001b[0m     ner_tags \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_at_least_two_common_elements(ner_tags):\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/datasets/arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/datasets/arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2787\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2788\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/datasets/formatting/formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/datasets/formatting/formatting.py:441\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m/mnt/lustre/yuxin/anaconda3/envs/sc4002/lib/python3.8/site-packages/datasets/formatting/formatting.py:147\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "lists2 = [0,1,2] # ['B-LOC', 'B-MISC', 'B-ORG']\n",
    "\n",
    "def has_at_least_two_common_elements(list1, list2=[0,1,2]):\n",
    "    common_elements = [value for value in list1 if value in list2]\n",
    "    return len(common_elements) >= 2\n",
    "\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    tokens = train_dataset['tokens'][i]\n",
    "    ner_tags = train_dataset['ner_tags'][i]\n",
    "\n",
    "    if has_at_least_two_common_elements(ner_tags):\n",
    "        print(i)\n",
    "        print(tokens)\n",
    "        print(ner_tags)\n",
    "        print([label_list[tag] for tag in ner_tags])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82114d-a819-4908-8143-0087bcb9938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [label_list[tag] for tag in train_dataset[5969]['ner_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27fe1611-8fe9-45aa-a8df-ad2e8d8a7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Swiss', 'Grand Prix', 'World Cup'],\n",
       " ['I-MISC', 'B-MISC I-MISC', 'B-MISC I-MISC'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (b)\n",
    "def form_complete_ne(dataset, i):\n",
    "    # define sets of tags\n",
    "    begin_tags = {'B-LOC', 'B-ORG', 'B-MISC'}\n",
    "    inside_tags = {'I-ORG', 'I-LOC', 'I-PER', 'I-MISC'}\n",
    "    outside_tags = {'O'}\n",
    "\n",
    "    words = []\n",
    "    word = []\n",
    "    entities = []\n",
    "    entity = []\n",
    "\n",
    "    tokens = dataset['tokens'][i]\n",
    "    ner_tags = dataset['ner_tags'][i]\n",
    "\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        tag = label_list[tag]\n",
    "\n",
    "        if (tag in begin_tags or tag in outside_tags) and word:\n",
    "            words.append(' '.join(word))\n",
    "            entities.append(' '.join(entity))\n",
    "            word = []\n",
    "            entity = []\n",
    "\n",
    "        if tag in begin_tags or tag in inside_tags:\n",
    "            word.append(token)\n",
    "            entity.append(tag)\n",
    "\n",
    "    if word:\n",
    "        words.append(' '.join(word))\n",
    "        entities.append(' '.join(entity))\n",
    "\n",
    "    return words, entities\n",
    "\n",
    "form_complete_ne(train_dataset, 5969)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7891e6-272f-49fa-9d63-19049f999a10",
   "metadata": {},
   "source": [
    "# 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db447bac-3109-4c13-8257-3bca386dde26",
   "metadata": {},
   "source": [
    "1. `<PAD>` Token:\n",
    "\n",
    "This token is typically initialized to a zero vector because it's meant to be a neutral padding value that doesn't interfere with computation\n",
    "\n",
    "2. `<UNK>` Token:\n",
    "- Zero Vector: Similar to the <PAD> token, you can initialize it to a zero vector.\n",
    "- Average Vector: Initialize it as the average of all word vectors in your pretrained embeddings. This gives it a kind of \"average\" representation of the language.\n",
    "- Random Vector: Randomly initialize it, which might add some noise and robustness to the embeddi\n",
    "\n",
    "For many tasks, initializing the <UNK> token as the average of all word vectors works well. It makes the <UNK> token have a representation that is, on average, similar to any random word from the vocabulary, which can be beneficial since the <UNK> token is used for words that aren't in the training vocabulary but could be anywhere in the semantic space.ngs.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c36319-becd-4f14-be61-614cc35852e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether <UNK> in w2v: False\n",
      "whether <PAD> in w2v: False\n",
      "word2idx['<UNK>']: 3000000\n",
      "word2idx['<PAD>']: 3000001\n",
      "after insert UNK:  (3000001, 300)\n",
      "after insert UNK:  (3000002, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Out-of-vocabulary (OOV) words\n",
    "# 1. can be replaced with a special token, such as \"<OOV>\" or \"<UNK>\".\n",
    "# 2. can be ignored.\n",
    "\n",
    "word2idx = w2v.key_to_index\n",
    "print(f\"whether <UNK> in w2v: {'<UNK>' in word2idx}\") # False\n",
    "print(f\"whether <PAD> in w2v: {'<PAD>' in word2idx}\") # False\n",
    "\n",
    "# Add '<UNK>' and '<PAD>' tokens to the vocabulary index\n",
    "word2idx['<UNK>'] = len(word2idx)\n",
    "word2idx['<PAD>'] = len(word2idx)\n",
    "\n",
    "print(f\"word2idx['<UNK>']: {word2idx['<UNK>']}\")\n",
    "print(f\"word2idx['<PAD>']: {word2idx['<PAD>']}\")\n",
    "\n",
    "# add the '<UNK>' word to the vocabulary of the Word2Vec model\n",
    "# initialize it with the average of all word vectors in the pretrained embeddings.\n",
    "unk_vector = np.mean(w2v.vectors, axis=0)\n",
    "w2v.vectors = np.vstack([w2v.vectors, unk_vector])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n",
    "\n",
    "# add the '<PAD>' word to the vocabulary of the Word2Vec model\n",
    "# initialize it with a row of zeros in the vectors matrix.\n",
    "w2v.vectors = np.vstack([w2v.vectors, np.zeros(w2v.vectors[0].shape)])\n",
    "print(\"after insert UNK: \", w2v.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6fbbae-6788-4067-85c8-23ed010199e6",
   "metadata": {},
   "source": [
    "# Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3beb52ff-de17-4f67-899c-6b7e8dc58802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to Indices\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.get(word, vocab.get('<UNK>')) for word in sentence]\n",
    "\n",
    "tag2idx = {\n",
    "    'B-LOC': 0,\n",
    "    'B-MISC': 1,\n",
    "    'B-ORG': 2,\n",
    "    'I-LOC': 3,\n",
    "    'I-MISC': 4,\n",
    "    'I-ORG': 5,\n",
    "    'I-PER': 6,\n",
    "    'O': 7,\n",
    "    'PAD': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55757fa-5020-481d-b1c1-ba3faa21062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, vocab):\n",
    "        self.sentences = [torch.tensor(sentence_to_indices(sentence, vocab)) for sentence in sentences]\n",
    "        self.tags = [torch.tensor(tag) for tag in tags]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = NERDataset(train_dataset['tokens'], train_dataset['ner_tags'], word2idx)\n",
    "validation_dataset = NERDataset(validation_dataset['tokens'], validation_dataset['ner_tags'], word2idx)\n",
    "test_dataset = NERDataset(test_dataset['tokens'], test_dataset['ner_tags'], word2idx)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word2idx['<PAD>'])\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=tag2idx['PAD'])\n",
    "    return sentences_padded, tags_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d01a3-0621-413d-b675-ae79a6ff8866",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cee6066-7544-4b7b-97b5-418a7788f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(w2v.vectors)\n",
    "\n",
    "class BiLSTMNERModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_of_layers, output_dim):\n",
    "        super(BiLSTMNERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, padding_idx=word2idx['<PAD>'], freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_of_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        tag_scores = torch.log_softmax(tag_space, dim=-1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0cb07-a595-46c5-bf81-9331df1ba55c",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a66728d-97bf-4e65-a9e6-304671240573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_DIM: 300\n",
      "VOCAB_SIZE: 3000002\n",
      "TAGSET_SIZE: 9\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = w2v[0].shape[0]\n",
    "print(f\"EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "HIDDEN_DIM = 150\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "print(f\"TAGSET_SIZE: {TAGSET_SIZE}\")\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "def idx_to_tags(indices):\n",
    "    return [idx2tag[idx] for idx in indices]\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def early_stop(self, f1):\n",
    "        if f1 > self.max_f1:\n",
    "            self.max_f1 = f1\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7646145-48f2-45ce-a251-ba23bf879dd7",
   "metadata": {},
   "source": [
    "# Test device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7af648-2fc1-47de-b63e-6a28f1cc36cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d1f82-b9ee-4532-8a5b-37b917992ec7",
   "metadata": {},
   "source": [
    "# Hyperparamter tuning\n",
    "https://necromuralist.github.io/Neurotic-Networking/posts/nlp/ner-evaluating-the-model/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b544883d-0f34-4044-8916-d09c8029f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_loader, device):\n",
    "    # print(next(model.parameters()).device) \n",
    "    print(\"evaluate.....\")\n",
    "    # Evaluate on the validation dataset\n",
    "    # Placeholder to store true and predicted tags\n",
    "    y_true = [] # true tags\n",
    "    y_pred = [] # predicted tags\n",
    "        \n",
    "    # Evaluate the model on the validation dataset\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in validation_loader:\n",
    "            # Move the data to the GPU\n",
    "            sentences, tags = sentences.to(device), tags.to(device)\n",
    "            tag_scores = model(sentences)\n",
    "            # print(tag_scores.device)\n",
    "            predictions = tag_scores.argmax(dim=-1).tolist()\n",
    "            # print(predictions)\n",
    "                \n",
    "            # Convert index to tags\n",
    "            # Note: filtering out padding tokens\n",
    "            for sentence, true_seq, pred_seq in zip(sentences, tags.tolist(), predictions):\n",
    "                valid_length = (sentence != word2idx['<PAD>']).sum().item()\n",
    "                true_tags = [idx2tag[idx] for idx in true_seq[:valid_length]]\n",
    "                pred_tags = [idx2tag[idx] for idx in pred_seq[:valid_length]]\n",
    "                y_true.append(true_tags)\n",
    "                y_pred.append(pred_tags)\n",
    "        \n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763cda2-3da9-462c-980a-78c8791f8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "Resuming from 127 existing results.\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 128, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.01, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.001, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 256, 'learning_rate': 0.0001, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.1, 'num_layers': 3, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 1, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.sgd.SGD'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.adagrad.Adagrad'>}\n",
      "Skipping already evaluated combination: {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.adam.Adam'>}\n",
      "================================================\n",
      " {'batch_size': 16, 'hidden_dim': 512, 'learning_rate': 0.01, 'num_layers': 2, 'optimizer': <class 'torch.optim.rmsprop.RMSprop'>}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from torch.optim import SGD, Adagrad, Adam, RMSprop\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB1\n",
    "import gc  # Python's garbage collector interface\n",
    "import json\n",
    "import os\n",
    "\n",
    "    \n",
    "# Define the range of hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32],\n",
    "    'optimizer': [SGD, Adagrad, Adam, RMSprop],\n",
    "    'hidden_dim': [128, 256, 512],\n",
    "    'num_layers': [1, 2, 3],\n",
    "}\n",
    "\n",
    "# Check for a GPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# Function to save the best model to CPU and clear memory\n",
    "def save_and_clear_best_model(current_best_model, best_state):\n",
    "    # Save the model state dictionary\n",
    "    torch.save(current_best_model.state_dict(), best_state)\n",
    "    # Delete model and empty cache\n",
    "    del current_best_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "best_model_state = 'best_model_state.pth'\n",
    "\n",
    "# placeholder for the best model and best F1 score\n",
    "best_model = None\n",
    "best_f1_score = 0\n",
    "best_params = None\n",
    "\n",
    "# Load existing results if they exist\n",
    "if os.path.isfile('hyperparameter_tuning_results.json'):\n",
    "    with open('hyperparameter_tuning_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    existing_combinations = {(result['learning_rate'], result['batch_size'], result['optimizer'], result['hidden_dim'], result['num_layers']) for result in results}\n",
    "    print(f\"Resuming from {len(results)} existing results.\")\n",
    "else:\n",
    "    results = []\n",
    "    existing_combinations = set()\n",
    "    \n",
    "# Iterate over all combinations\n",
    "for params in list(ParameterGrid(param_grid)):\n",
    "    # Skip if this combination has already been evaluated\n",
    "    if (params['learning_rate'], params['batch_size'], params['optimizer'].__name__, params['hidden_dim'], params['num_layers']) in existing_combinations:\n",
    "        print(\"Skipping already evaluated combination:\", params)\n",
    "        continue\n",
    "        \n",
    "    # set up model\n",
    "    model = BiLSTMNERModel(EMBEDDING_DIM, params['hidden_dim'], params['num_layers'], TAGSET_SIZE).to(device)\n",
    "    loss_function = nn.NLLLoss(ignore_index=tag2idx['PAD']).to(device)\n",
    "    optimizer = params['optimizer'](model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    # Initialize dataloaders\n",
    "    train_loader = DataLoader(train_dataset, params['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "    validation_loader = DataLoader(validation_dataset, params['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize early stopper\n",
    "    early_stopper = EarlyStopper()\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"================================================\\n\", params)\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        total_loss = 0\n",
    "        model.train()  # Make sure the model is in training mode\n",
    "        for sentences, tags in train_loader:\n",
    "            sentences, tags = sentences.to(device), tags.to(device)  # Move data to GPU\n",
    "            model.zero_grad()\n",
    "            tag_scores = model(sentences)\n",
    "            loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        f1_validation = evaluate(model, validation_loader, device)  # You need to implement this function\n",
    "        print(f\"F1 Score (Epoch {epoch+1}): {f1_validation}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopper.early_stop(f1_validation):\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "    # Record the results\n",
    "    new_result = {\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'optimizer': params['optimizer'].__name__,\n",
    "        'hidden_dim': params['hidden_dim'],\n",
    "        'num_layers': params['num_layers'],\n",
    "        'final_epoch': epoch,\n",
    "        'f1_score': f1_validation\n",
    "    }\n",
    "    results.append(new_result)\n",
    "    # Save the results to the JSON file after each iteration\n",
    "    with open('hyperparameter_tuning_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        \n",
    "    # Keep track of the best model\n",
    "    if f1_validation > best_f1_score:\n",
    "        best_f1_score = f1_validation\n",
    "        best_params = params\n",
    "        print(f\"New best F1 score: {best_f1_score}\")\n",
    "        print(f\"Best hyperparameters: {best_params}\")\n",
    "        \n",
    "    # Clean up after each model is evaluated to free GPU memory\n",
    "    del model\n",
    "    gc.collect()  # Clean up garbage\n",
    "    torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "    print(\"================================================\")\n",
    "    \n",
    "# sort results by best F1 score\n",
    "results = sorted(results, key=lambda x: x['f1_score'], reverse=True)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3849fda9-b4d9-4eac-8bbc-b57d43f26026",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save the results to a JSON file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameter_tuning_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mresults\u001b[49m, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the best model state to a file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# torch.save(best_model_state, 'best_model.pth')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('hyperparameter_tuning_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Save the best model state to a file\n",
    "# torch.save(best_model_state, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357106ab-794d-46b3-9ae6-30025713a793",
   "metadata": {},
   "source": [
    "# Test the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb37fd0-79a5-4430-893e-269bd6ab2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder to store true and predicted tags for the test set\n",
    "y_true_test = []\n",
    "y_pred_test = []\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for sentences, tags in test_loader:\n",
    "        predictions = best_model(sentences)\n",
    "        \n",
    "        # Convert index to tags\n",
    "        # Note: filtering out padding tokens\n",
    "        for sentence, true_seq, pred_seq in zip(sentences, tags.tolist(), predictions):\n",
    "            valid_length = (sentence != word2idx['<PAD>']).sum().item()\n",
    "            true_tags = [idx2tag[idx] for idx in true_seq[:valid_length]]\n",
    "            pred_tags = [idx2tag[idx] for idx in pred_seq[:valid_length]]\n",
    "            y_true_test.append(true_tags)\n",
    "            y_pred_test.append(pred_tags)\n",
    "\n",
    "# Compute F1 score for the test set\n",
    "f1_test = f1_score(y_true_test, y_pred_test)\n",
    "report_test = classification_report(y_true_test, y_pred_test)\n",
    "\n",
    "print(\"F1 Score on Test Set:\", f1_test)\n",
    "print(\"Classification Report on Test Set:\\n\", report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66025b7d-179a-47ad-8c6d-878519b04d12",
   "metadata": {},
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b4dc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training\n",
      "Epoch 1, Loss: 211.2243566084653\n",
      "F1 Score on Test Set: 0.8203576830678887\n",
      "Epoch 2, Loss: 85.49815228581429\n",
      "F1 Score on Test Set: 0.8310655592860929\n",
      "Epoch 3, Loss: 63.95651657227427\n",
      "F1 Score on Test Set: 0.8478935698447895\n",
      "Epoch 4, Loss: 49.66351771284826\n",
      "F1 Score on Test Set: 0.8482000354672814\n",
      "Epoch 5, Loss: 38.52881736995187\n",
      "F1 Score on Test Set: 0.8559714795008913\n",
      "Epoch 6, Loss: 31.395666539494414\n",
      "F1 Score on Test Set: 0.8584579976985041\n",
      "Epoch 7, Loss: 24.271331545722205\n",
      "F1 Score on Test Set: 0.8539925306775743\n",
      "Epoch 8, Loss: 19.308925223827828\n",
      "F1 Score on Test Set: 0.8639305026150165\n",
      "Epoch 9, Loss: 17.019748357968638\n",
      "F1 Score on Test Set: 0.8573200992555832\n",
      "Epoch 10, Loss: 13.347994626405125\n",
      "F1 Score on Test Set: 0.8584507042253522\n",
      "Epoch 11, Loss: 12.555585913782124\n",
      "F1 Score on Test Set: 0.8533427912750488\n",
      "Epoch 12, Loss: 11.178530612298346\n",
      "F1 Score on Test Set: 0.8586966211999643\n",
      "Epoch 13, Loss: 9.511184494767804\n",
      "F1 Score on Test Set: 0.8604405286343613\n",
      "F1 Score on Test Set: 0.8604405286343613\n"
     ]
    }
   ],
   "source": [
    "# Final training\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "model = BiLSTMNERModel(EMBEDDING_DIM, optimal_hidden_dim, optimal_num_of_layers, TAGSET_SIZE)\n",
    "loss_function = nn.NLLLoss(ignore_index=tag2idx['PAD'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Final Training\")\n",
    "\n",
    "merged_train_dataset = ConcatDataset([train_dataset, validation_dataset])\n",
    "merged_train_loader = DataLoader(merged_train_dataset, optimal_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for sentences, tags in merged_train_loader:\n",
    "        tag_scores = model(sentences)\n",
    "        loss = loss_function(tag_scores.view(-1, TAGSET_SIZE), tags.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "    f1_test = test(model, optimal_batch_size)\n",
    "    if early_stopper.early_stop(f1_test):\n",
    "        break\n",
    "print(\"F1 Score on Test Set:\", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04df04-e0c1-46dd-86db-6c85d1418ce8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc649952-2d63-40bd-87cf-56b6e68f2268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def infer(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    token_indices = torch.tensor([sentence_to_indices(tokens, word2idx)])\n",
    "\n",
    "    # Get predictions from the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tag_scores = model(token_indices)\n",
    "        predictions = tag_scores.argmax(dim=-1).tolist()[0]\n",
    "\n",
    "    # Convert index to tags\n",
    "    predicted_tags = idx_to_tags(predictions, {v: k for k, v in tag2idx.items()})\n",
    "\n",
    "    \"\"\"\n",
    "    # Display the results\n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        print(f\"{token}: {tag}\")\n",
    "    \"\"\"\n",
    "    # Prepare aligned output\n",
    "    token_line = \"\"\n",
    "    tag_line = \"\"\n",
    "    for token, tag in zip(tokens, predicted_tags):\n",
    "        space_padding = max(len(token), len(tag)) + 2  # +2 to add some space between words for better readability\n",
    "        token_line += token.ljust(space_padding)\n",
    "        tag_line += tag.ljust(space_padding)\n",
    "\n",
    "    # Display the results\n",
    "    print(token_line)\n",
    "    print(tag_line)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"EU rejects German call to boycott British lamb .\"\n",
    "sentence = \"Barack Obama was born in Hawaii and worked as the President of the United States.\"\n",
    "infer(sentence)\n",
    "\n",
    "sentence = \"Jiang Yuxin was born in Shenyang and is now a student in Nanyang Technological University.\"\n",
    "infer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045ea8-7f99-40c0-9d46-48433cb9dc61",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "e.g. f1 score per class: https://medium.com/illuin/named-entity-recognition-with-bilstm-cnns-632ba83d3d41\n",
    "## data report\n",
    "https://github.com/senadkurtisi/pytorch-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093241f-ed65-4106-8920-81120c4c9626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002",
   "language": "python",
   "name": "sc4002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
