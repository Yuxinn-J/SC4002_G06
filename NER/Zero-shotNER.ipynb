{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f62925-5a23-4b8f-98d7-cc06b1bd28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up prompts\n",
    "SYSTEM_PROMPT = \"You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities you need to extract, the sentence from where your extract the entities and the output format with examples.\"\n",
    "\n",
    "USER_PROMPT_1 = \"Are you clear about your role?\"\n",
    "\n",
    "ASSISTANT_PROMPT_1 = \"Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started.\"\n",
    "\n",
    "GUIDELINES_PROMPT = (\n",
    "    \"Entity Definition:\\n\"\n",
    "    \"1. PERSON: Short name or full name of a person from any geographic regions.\\n\"\n",
    "    \"2. DATE: Any format of dates. Dates can also be in natural language.\\n\"\n",
    "    \"3. LOC: Name of any geographic location, like cities, countries, continents, districts etc.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Output Format:\\n\"\n",
    "    \"{{'PERSON': [list of entities present], 'DATE': [list of entities present], 'LOC': [list of entities present]}}\\n\"\n",
    "    \"If no entities are presented in any categories keep it None\\n\"\n",
    "    \"\\n\"\n",
    "    \"Examples:\\n\"\n",
    "    \"\\n\"\n",
    "    \"1. Sentence: Mr. Jacob lives in Madrid since 12th January 2015.\\n\"\n",
    "    \"Output: {{'PERSON': ['Mr. Jacob'], 'DATE': ['12th January 2015'], 'LOC': ['Madrid']}}\\n\"\n",
    "    \"\\n\"\n",
    "    \"2. Sentence: Mr. Rajeev Mishra and Sunita Roy are friends and they meet each other on 24/03/1998.\\n\"\n",
    "    \"Output: {{'PERSON': ['Mr. Rajeev Mishra', 'Sunita Roy'], 'DATE': ['24/03/1998'], 'LOC': ['None']}}\\n\"\n",
    "    \"\\n\"\n",
    "    \"3. Sentence: {}\\n\"\n",
    "    \"Output: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcc63d-16f4-48ae-92cc-45b540a34493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 setup\n",
    "import openai\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def openai_chat_completion_response(final_prompt):\n",
    "  response = openai.ChatCompletion.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": USER_PROMPT_1},\n",
    "                    {\"role\": \"assistant\", \"content\": ASSISTANT_PROMPT_1},\n",
    "                    {\"role\": \"user\", \"content\": final_prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "  return response['choices'][0]['message']['content'].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5980a5c-90cc-4f27-b623-46957abe23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# read data\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data\n",
    "\n",
    "# prepare data\n",
    "def convert_to_dataset(data):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": [], \"sentence\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [token_data[3] for token_data in sentence]\n",
    "        sentence_str = \" \".join(tokens)\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "        formatted_data[\"sentence\"].append(sentence_str)\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "test_data = read_conll_file(\"/mnt/lustre/yuxin/SC4002_G06/datasets/CoNLL2003/eng.testb\")\n",
    "test_dataset = convert_to_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f49da6-c134-47f7-8ac0-9ce174765a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['SOCCER',\n",
       "  '-',\n",
       "  'JAPAN',\n",
       "  'GET',\n",
       "  'LUCKY',\n",
       "  'WIN',\n",
       "  ',',\n",
       "  'CHINA',\n",
       "  'IN',\n",
       "  'SURPRISE',\n",
       "  'DEFEAT',\n",
       "  '.'],\n",
       " 'ner_tags': ['O',\n",
       "  'O',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " 'sentence': 'SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fba49-1c30-4ad5-b997-535b1ee66aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4002",
   "language": "python",
   "name": "sc4002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
